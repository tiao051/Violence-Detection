

BỘ CÔNG THƯƠNG
TRƯỜNG ĐẠI HỌC CÔNG THƯƠNG TP.HCM
KHOA CÔNG NGHỆ THÔNG TIN


 






 KHÓA LUẬN CỬ NHÂN

	
ĐỀ TÀI: ỨNG DỤNG AI PHÁT HIỆN BẠO LỰC THỜI GIAN THỰC TỪ VIDEO TRÊN NỀN TẢNG WEB VÀ MOBILE
	





	Giảng viên hướng dẫn: TS. Ngô Dương Hà
	Sinh viên thực hiện:
		1. 2001224971 – Nguyễn Minh Thọ
		2. 2001224865 – Đỗ Cao Thắng
		3. 2001224555 – Nguyễn Hoàng Tuấn
	        		
	


	TP. HỒ CHÍ MINH – 2025
 
BỘ CÔNG THƯƠNG
TRƯỜNG ĐẠI HỌC CÔNG THƯƠNG TP.HCM
KHOA CÔNG NGHỆ THÔNG TIN


 






 KHÓA LUẬN CỬ NHÂN

	
ĐỀ TÀI: ỨNG DỤNG AI PHÁT HIỆN BẠO LỰC THỜI GIAN THỰC TỪ VIDEO TRÊN NỀN TẢNG WEB VÀ MOBILE
	





	Giảng viên hướng dẫn: TS. Ngô Dương Hà
	Sinh viên thực hiện:
		1. 2001224971 – Nguyễn Minh Thọ
		2. 2001224865 – Đỗ Cao Thắng
		3. 2001224555 – Nguyễn Hoàng Tuấn

	        		
	


	TP. HỒ CHÍ MINH – 2025
 
BẢNG PHÂN CÔNG CÔNG VIỆC
STT	Họ Tên	MSSV	Công việc hoàn thành
1	Nguyễn Minh Thọ	2001224971	- Nghiên cứu cơ sở lý thuyết về nhận dạng hành động và Deep Learning.
- Xây dựng và huấn luyện mô hình AI (SME + MobileNetV3).
- Phát triển Admin Dashboard
- Phát triển Backend (API, WebSocket, Worker, Pub/Sub).
- Tích hợp hệ thống AI vào ứng dụng thực tế.
- Thực nghiệm, đánh giá kết quả và tối ưu hóa hệ thống.
- Viết báo cáo: Mở đầu, Chương 1, Chương 2, Kết luận.
2	Đỗ Cao Thắng	2001224865	- Phân tích thiết kế hệ thống và cơ sở dữ liệu.
- Phát triển Mobile App Flutter.
- Thu thập và xử lý dữ liệu (RWF-2000, Hockey Fight, UVD).
- Tích hợp hệ thống AI vào ứng dụng thực tế.
- Thực nghiệm, đánh giá kết quả và tối ưu hóa hệ thống.
- Viết báo cáo: Mở đầu, Chương 3, Chương 4, Kết luận.
3	Nguyễn Hoàng Tuấn	2001224555	
 
LỜI CẢM ƠN
	Lời đầu tiên, chúng em xin gửi lời cảm ơn chân thành và sâu sắc nhất đến TS. Ngô Dương Hà, người thầy đã tận tình hướng dẫn, chỉ bảo và truyền đạt những kiến thức quý báu cho chúng em trong suốt quá trình thực hiện khóa luận này. Thầy không chỉ là người định hướng về mặt học thuật mà còn là nguồn động viên tinh thần to lớn, giúp chúng em vượt qua những khó khăn, bế tắc trong quá trình nghiên cứu. Sự kiên nhẫn và tâm huyết của Thầy chính là kim chỉ nam để chúng em hoàn thành đề tài một cách tốt nhất.
	Chúng em cũng xin gửi lời tri ân đến quý Thầy, Cô trong Khoa Công nghệ Thông tin - Trường Đại học Công Thương TP.HCM. Những bài giảng tâm huyết, những kiến thức nền tảng vững chắc mà quý Thầy, Cô đã trang bị trong suốt 4 năm học vừa qua chính là hành trang vô giá để chúng em tự tin bước vào thực hiện luận án này cũng như trên con đường sự nghiệp sau này.
	Mặc dù đã rất cố gắng, nhưng do giới hạn về kiến thức và thời gian, khóa luận chắc chắn không tránh khỏi những thiếu sót. Chúng em rất mong nhận được những ý kiến đóng góp quý báu của quý Thầy, Cô để đề tài được hoàn thiện hơn.
	Chúng em xin chân thành cảm ơn
Sinh viên thực hiện

Sinh viên 1 			Sinh viên 2 			Sinh viên 3
	   Ký và ghi rõ họ tên                   Ký và ghi rõ họ tên                  Ký và ghi rõ họ tên 

         		               

     Nguyễn Minh Thọ                         Đỗ Cao Thắng                    Nguyễn Hoàng Tuấn










NHẬN XÉT CỦA GIẢNG VIÊN HƯỚNG DẪN
Khóa luận “Ứng dụng AI phát hiện bạo lực thời gian thực từ video trên nền tảng Web và Mobile” được sinh viên thực hiện đề tài một cách nghiêm túc, có tinh thần học tập tốt và chủ động trong việc nghiên cứu cũng như triển khai hệ thống. Đề tài mang tính ứng dụng thực tiễn cao, phù hợp với nhu cầu giám sát an ninh hiện nay, đặc biệt trong bối cảnh số lượng camera tăng lên nhưng khả năng theo dõi thủ công còn nhiều hạn chế.
Nội dung khóa luận được triển khai đầy đủ, từ phân tích tổng quan bài toán, mô hình lý thuyết về thị giác máy tính và học sâu, cho đến việc đề xuất kiến trúc AI ba module (SME – STE – GTE) tối ưu vừa đảm bảo độ chính xác vừa đáp ứng yêu cầu thời gian thực. Phần xây dựng mô hình rõ ràng, trích xuất đặc trưng và phân tích thời gian.
Hệ thống được xây dựng hoàn chỉnh, bao gồm backend xử lý video thời gian thực, hệ thống cảnh báo, dashboard quản trị và ứng dụng mobile. Các thí nghiệm được thực hiện bài bản với nhiều bộ dữ liệu khác nhau (RWF-2000, Hockey Fight, UVD) và có so sánh với các nghiên cứu liên quan.
Đề nghị cho phép đề tài khóa luận này được bảo vệ trước hội đồng.

	
     Tp Hồ Chí Minh, ngày … tháng … năm …
	                 GIẢNG VIÊN HƯỚNG DẪN
	                 (Ký và ghi rõ họ tên)

				           		         	
             TS. Ngô Dương Hà



MỤC LỤC
BẢNG PHÂN CÔNG CÔNG VIỆC	i
LỜI CẢM ƠN	ii
NHẬN XÉT CỦA GIẢNG VIÊN HƯỚNG DẪN	iii
MỤC LỤC	iv
DANH MỤC CÁC KÝ HIỆU VÀ CHỮ VIẾT TẮT	vii
DANH MỤC HÌNH	ix
DANH MỤC BẢNG	xi
MỞ ĐẦU	1
CHƯƠNG 1:	TỔNG QUAN	3
1.1	GIỚI THIỆU	3
1.2	HƯỚNG TIẾP CẬN	3
1.3	KHÓ KHĂN VÀ THÁCH THỨC	5
1.4	HƯỚNG GIẢI QUYẾT	7
1.4.1	Áp dụng kiến trúc AI ba module chuyên biệt	7
1.4.2	Xây dựng hệ thống dưới dạng kiến trúc Module hoá:	8
1.4.3	Phát triển giao diện người dùng đa nền tảng:	8
1.5	TẦM NHÌN: TỪ PHÁT HIỆN SỰ CỐ ĐẾN CHỦ ĐỘNG PHÒNG NGỪA	9
CHƯƠNG 2:	CƠ SỞ LÝ THUYẾT	9
2.1	TỔNG QUAN VỀ THỊ GIÁC MÁY TÍNH TRONG BÀI TOÁN NHẬN DẠNG HÀNH ĐỘNG	9
2.1.1	Giới thiệu về Thị giác máy tính	9
2.1.2	Bài toán Nhận dạng hành động con người	10
2.2	MẠNG NƠ-RON TÍCH CHẬP (CONVOLUTIONAL NEURAL NETWORK - CNN)	11
2.2.1	Giới thiệu	11
2.2.2	Các thành phần chính của Mạng CNN	11
2.3	CÁC KIẾN TRÚC CNN CHO XỬ LÝ VIDEO	14
2.3.1	Sử dụng Mạng 2D-CNN cho Video	14
2.3.2	Mạng 3D-CNN (3D Convolutional Neural Networks)	15
2.3.3	Giới thiệu MobileNetV2 – Kiến trúc gọn nhẹ và hiệu quả	16
2.4	CƠ SỞ LÝ THUYẾT CỦA MÔ HÌNH ĐỀ XUẤT	17
2.4.1	Module Trích xuất Chuyển động Không gian (SME)	18
2.4.2	Module Trích xuất Đặc trưng Thời gian Ngắn (STE)	20
2.4.3	Module Trích xuất Đặc trưng Thời gian Toàn cục (GTE)	21
2.5	LỰA CHỌN VÀ TỐI ƯU HÓA MẠCH XƯƠNG SỐNG (BACKBONE) CHO MÔ HÌNH ĐỀ XUẤT	22
2.6	CÁC CHỈ SỐ ĐÁNH GIÁ MÔ HÌNH	23
2.6.1	Độ chính xác (Accuracy)	24
2.6.2	Độ chính xác dự báo (Precision)	24
2.6.3	Độ thu hồi (Recall / Sensitivity)	25
2.6.4	F1-Score	25
2.7	CƠ SỞ LÝ THUYẾT VỀ PHÂN TÍCH DỮ LIỆU VÀ KIẾN TRÚC HỆ THỐNG	25
2.7.1	Học máy trong Phân tích dữ liệu (Data Analytics)	25
2.7.2	Thuật toán Random Forest (Rừng ngẫu nhiên)	26
2.7.3	Thuật toán K-Means Clustering (Phân cụm K-Means)	26
2.7.4	Kiến trúc hướng sự kiện (Event-Driven Architecture) với Kafka	27
2.8	NỀN TẢNG PHÁT TRIỂN ỨNG DỤNG DI ĐỘNG	28
2.8.1	Flutter - Framework phát triển ứng dụng đa nền tảng	28
2.8.2	Firebase - Nền tảng Backend-as-a-Service (BaaS)	28
2.8.3	Thư viện phát video: video_player	29
CHƯƠNG 3:	XÂY DỰNG THUẬT TOÁN VÀ ỨNG DỤNG	29
3.1	TỔNG QUAN VỀ LUỒNG XỬ LÝ (PIPELINE)	29
3.2	BỘ DỮ LIỆU THỰC NGHIỆM	30
3.2.1	Tổng quan về bộ dữ liệu	30
3.2.2	Bộ dữ liệu hợp nhất (Unified Violence Dataset - UVD)	35
3.2.3	Quy trình xử lý dữ liệu	36
3.3	CÁC THUẬT TOÁN ĐƯỢC SỬ DỤNG	38
3.3.1	Chi tiết thuật toán lõi	38
3.3.2	Các thuật toán cho Suy luận	41
3.3.3	Các thuật toán cho Huấn luyện	44
3.4	MODULE PHÂN TÍCH VÀ DỰ BÁO (ANALYTICS ENGINE)	49
3.4.1	Quy trình xử lý dữ liệu Analytics	49
3.4.2	Thuật toán Phân cụm (K-Means) triển khai	50
3.4.3	Thuật toán Dự báo (Random Forest) triển khai	50
3.5	THIẾT KẾ LOGIC ỨNG DỤNG	51
3.5.1	Logic xử lý tại Backend (Application Logic)	51
3.5.2	Kiến trúc Microservices và Xử lý dữ liệu lớn	51
3.5.3	Logic hiển thị và tương tác (Frontend Logic)	52
CHƯƠNG 4:	THỰC NGHIỆM VÀ KẾT QUẢ	52
4.1	MÔI TRƯỜNG THỰC NGHIỆM	52
4.1.1	Cấu hình phần cứng	52
4.1.2	Cấu hình phần mềm	52
4.1.3	Hạ tầng triển khai	52
4.1.4	Tham số huấn luyện	52
4.2	KẾT QUẢ THỰC NGHIỆM CHI TIẾT	53
4.2.1	So sánh hiệu năng các Backbone	53
4.2.2	Kết quả trên tập dữ liệu RWF-2000	53
4.2.3	Kết quả trên tập dữ liệu Hockey Fight	54
4.2.4	Kết quả trên bộ dữ liệu hợp nhất (UVD)	55
4.2.5	Kết quả ứng dụng thực tế	57
4.3	SO SÁNH VỚI CÁC NGHIÊN CỨU LIÊN QUAN	58
4.3.1	Đánh giá kết quả dựa trên bộ dữ liệu Hockey Fight	58
4.3.2	Đánh giá kết quả dựa trên bộ dữ liệu RWF – 2000	59
4.3.3	Đánh giá kết quả dựa trên bộ dữ liệu hợp nhất (UVD)	60
4.4	PHÂN TÍCH ĐỊNH TÍNH	60
4.5	TRIỂN KHAI ỨNG DỤNG THỰC TẾ	61
4.5.1	Giao diện Trung tâm hệ thống	61
4.5.2	Giao diện Lịch sử cảnh báo	61
4.5.3	Giao diện Danh sách Camera trên Mobile	62
4.5.4	Giao diện Danh sách Sự kiện trên Mobile	63
4.5.5	Giao diện Chi tiết Sự kiện và Phát lại	63
4.5.6	Giao diện Quản lý Tài khoản và Đăng nhập	64
CHƯƠNG 5:	KẾT LUẬN VÀ HƯỚNG PHÁT TRIỂN	67
5.1	KẾT LUẬN	67
5.2	HẠN CHẾ	67
5.3	HƯỚNG PHÁT TRIỂN	68
5.3.1	Tối ưu mô hình	68
5.3.2	Ứng dụng trên thiết bị biên	68
5.3.3	Mở rộng phạm vi bài toán	68
5.3.4	Phát triển hệ thống	69
5.3.5	Cải thiện khả năng học của mô hình	69
5.3.6	Triển khai thực tế	69
TÀI LIỆU THAM KHẢO	70

 
DANH MỤC CÁC KÝ HIỆU VÀ CHỮ VIẾT TẮT
Viết tắt	Tiếng Anh	Tiếng Việt
AI	Artificial Intelligence	Trí tuệ nhân tạo
API	Application Programming Interface	Giao diện lập trình ứng dụng
BaaS	Backend-as-a-Service	Dịch vụ hạ tầng backend
CCTV	Closed-Circuit Television	Camera quan sát (Truyền hình mạch kín)
CDN	Content Delivery Network	Mạng phân phối nội dung
CNN	Convolutional Neural Network	Mạng nơ-ron tích chập
CPU	Central Processing Unit	Bộ xử lý trung tâm
EDA	Event-Driven Architecture	Kiến trúc hướng sự kiện
FCM	Firebase Cloud Messaging	Dịch vụ nhắn tin đám mây Firebase
FLOPs	Floating Point Operations	Số phép toán dấu chấm động (Đo độ phức tạp tính toán)
FPS	Frames Per Second	Số khung hình trên giây
GAP	Global Average Pooling	Gộp trung bình toàn cục
GPU	Graphics Processing Unit	Bộ xử lý đồ họa
GTE	Global Temporal Extractor	Bộ trích xuất đặc trưng thời gian toàn cục
HAR	Human Action Recognition	Nhận dạng hành động con người
HDFS	Hadoop Distributed File System	Hệ thống tệp phân tán Hadoop
JSON	JavaScript Object Notation	Định dạng dữ liệu đối tượng JavaScript
JIT	Just-In-Time	Biên dịch tức thời
LSTM	Long Short-Term Memory	Bộ nhớ ngắn-dài hạn
MLP	Multilayer Perceptron	Mạng Perceptron đa lớp
NAS	Neural Architecture Search	Tìm kiếm kiến trúc mạng nơ-ron
ONVIF	Open Network Video Interface Forum	Diễn đàn giao diện video mạng mở (Chuẩn kết nối camera)
ReLU	Rectified Linear Unit	Đơn vị tuyến tính chỉnh lưu (Hàm kích hoạt)
RGB	Red Green Blue	Hệ màu đỏ, lục, lam
RLVS	Real Life Violence Situations	Bộ dữ liệu tình huống bạo lực thực tế
ROI	Region of Interest	Vùng quan tâm
RTSP	Real-Time Streaming Protocol	Giao thức truyền phát thời gian thực
RNN	Recurrent Neural Network	Mạng nơ-ron hồi quy
SME	Spatial Motion Extractor	Bộ trích xuất chuyển động không gian
STE	Short-term Spatiotemporal Extractor	Bộ trích xuất đặc trưng không gian-thời gian ngắn hạn
UI/UX	User Interface / User Experience	Giao diện người dùng / Trải nghiệm người dùng
UVD	Unified Violence Dataset	Bộ dữ liệu hợp nhất


 
DANH MỤC HÌNH
Hình 2.1: Minh họa bài toán Phát hiện đối tượng trong Thị giác máy tính	9
Hình 2.2: Minh hoạ bài toán Nhận dạng hành động con người (HAR)	10
Hình 2.3: Minh họa chi tiết phép toán tích chập 3D	12
Hình 2.4: Đồ thị hàm kích hoạt ReLU	13
Hình 2.5: Sơ đồ luồng xử lý cơ bản của một mạng CNN	13
Hình 2.6: So sánh giữa phép tích chập 2D và 3D	15
Hình 2.7: Minh họa kiến trúc Tích chập Tách biệt theo Chiều sâu (Depthwise Separable Convolution)	17
Hình 2.8: Module Trích xuất Chuyển động Không gian (SME)	18
Hình 2.9: Khoảng cách Euclidean giữa hai khung hình liên tiếp Ft, Ft+1	19
Hình 2.10: Phép toán hình thái học	19
Hình 2.11: Mt: Trích xuất không gian của chuyển động	20
Hình 2.12: Module Trích xuất Đặc trưng Thời gian Ngắn (STE)	20
Hình 2.13: Module Trích xuất Đặc trưng Thời gian Toàn cục (GTE)	21
Hình 3.1: Sơ đồ luồng xử lý tổng quát của hệ thống	29
Hình 3.2: Một số mẫu khung hình từ tập dữ liệu RWF-2000	31
Hình 3.3: Biểu đồ phân bố độ dài video RWF-2000	31
Hình 3.4: Một số mẫu khung hình từ tập dữ liệu Hockey Fight	32
Hình 3.5: Các chỉ số đặc trưng của hành vi bạo lực	33
Hình 3.6: Một số mẫu khung hình từ tập dữ liệu RLVS	33
Hình 3.7: Phân tích thống kê thời điểm xảy ra hành vi bạo lực trong tập dữ liệu RLVS	34
Hình 3.8: Một số mẫu khung hình từ tập dữ liệu UVD	35
Hình 3.9: Biểu đồ tròn thành phần bộ dữ liệu UVD	36
Hình 3.10: Biểu đồ phân bố nhãn UVD	36
Hình 3.11: Mã giả cho Giai đoạn 1	39
Hình 3.12: Mã giả cho Giai đoạn 2	40
Hình 3.13: Mã giả cho Giai đoạn 3	41
Hình 3.14: Mã giả cho Thuật toán Cửa sổ trượt và Quản lý bộ đệm	42
Hình 3.15: Thuật toán Hậu xử lý và Làm trơn kết quả	43
Hình 3.16: Mã giả cho Lập lịch xử lý bất đồng bộ	43
Hình 3.17: Mã giả cho Giải thuật tăng cường dữ liệu	44
Hình 3.18: Mã giả cho Vòng lặp huấn luyện	45
Hình 3.19: Mã giả cho Thuật toán tối ưu hoá	48
Hình 3.20: Mã giả cho Điều chỉnh tốc độ học	48
Hình 4.1: Biểu đồ Accuracy và Loss trong quá trình huấn luyện trên RWF-2000	54
Hình 4.2: Ma trận nhầm lẫn (Confusion Matrix) trên tập Test RWF-2000	54
Hình 4.3: Biểu đồ Accuracy và Loss trong quá trình huấn luyện trên Hockey Fight	55
Hình 4.4: Ma trận nhầm lẫn trên tập Hockey Fight	55
Hình 4.5: Biểu đồ Accuracy và Loss trên tập UVD	56
Hình 4.6: Ma trận nhầm lẫn trên tập Test	56
Hình 4.7: Giao diện Trung tâm hệ thống	61
Hình 4.8: Giao diện lịch sử cảnh báo	62
Hình 4.9: Giao diện Danh sách Camera	62
Hình 4.10: Giao diện Danh sách Sự kiện	63
Hình 4.11: Giao diện Chi tiết sự kiện và tính năng báo cáo sai	64
Hình 4.12: Giao diện Đăng nhập	65
Hình 4.13: Giao diện Đăng ký	65
Hình 4.14: Giao diện Quản lý hồ sơ cá nhân	66
 
DANH MỤC BẢNG
Bảng 2.1: Bảng so sánh Params, Accuracy và FLOPs giữa các Backbone (MobileNetV2, MobileNetV3, EfficientNet B0, MNasNet) trên RWF – 2000 và HockeyFight	23
Bảng 4.1: So sánh hiệu năng và tài nguyên giữa các Backbone	53
Bảng 4.2: Phân rã độ trễ toàn trình	57
Bảng 4.3: So sánh độ chính xác giữa các mô hình trên bộ dữ liệu Hockey Fight	58
Bảng 4.4: So sánh các thông số giữa các mô hình trên bộ dữ liệu RWF – 2000	59
Bảng 4.5: So sánh các thông số giữa các mô hình trên bộ dữ liệu UVD	60

 
MỞ ĐẦU
	Trong bối cảnh đô thị hóa và sự phát triển không ngừng của các không gian công cộng như trung tâm thương mại, trường học, bệnh viện, và bến xe, hệ thống camera giám sát an ninh (CCTV) đã trở thành một phần không thể thiếu, đóng vai trò xương sống trong việc duy trì trật tự và an toàn xã hội. Tuy nhiên, phương pháp giám sát truyền thống dựa vào con người, yêu cầu nhân viên an ninh phải theo dõi đồng thời hàng chục, thậm chí hàng trăm luồng video trực tiếp, đã bộc lộ những hạn chế cố hữu về khả năng tập trung, hiệu suất làm việc và tốc độ phản ứng. Các hành vi bạo lực, thường diễn ra trong tích tắc, có thể dễ dàng bị bỏ sót, dẫn đến những hậu quả đáng tiếc.
	Sự trỗi dậy của Trí tuệ Nhân tạo, đặc biệt trong lĩnh vực Thị giác máy tính và Học sâu, đã mang lại một bước đột phá, mở ra khả năng tự động hóa và thông minh hóa quy trình giám sát an ninh. Các hệ thống AI có thể phân tích video liên tục 24/7, nhận diện các mẫu hành vi bất thường và đưa ra cảnh báo tức thì với độ chính xác cao, giúp con người can thiệp kịp thời và hiệu quả.
Nắm bắt được tính cấp thiết đó, luận văn “Ứng dụng AI phát hiện bạo lực thời gian thực từ video trên nền tảng web và mobile” được thực hiện nhằm xây dựng một giải pháp toàn diện, góp phần giải quyết những thách thức của bài toán giám sát trong thực tiễn. Đề tài không chỉ tập trung vào việc nghiên cứu và áp dụng các mô hình AI tiên tiến mà còn hướng đến việc xây dựng một hệ thống hoàn chỉnh có tính ứng dụng cao.
Điểm cốt lõi trong cách tiếp cận của luận án là đề xuất và triển khai một kiến trúc học sâu được tối ưu hóa cho bài toán phát hiện hành vi. Thay vì sử dụng các phương pháp phát hiện đối tượng phức tạp và tốn kém tài nguyên tính toán, luận án tập trung vào việc trích xuất các vùng có chuyển động nổi bật trong video một cách hiệu quả. Từ đó, một mô hình phân loại gọn nhẹ sẽ phân tích các đặc trưng không gian-thời gian trong các vùng này để đưa ra kết luận về hành vi bạo lực. Cách tiếp cận này giúp cân bằng giữa độ chính xác và tốc độ xử lý, đáp ứng yêu cầu nghiêm ngặt về thời gian thực của các hệ thống giám sát.
Xuyên suốt quá trình thực hiện, đề tài đã giải quyết được các vấn đề cốt lõi sau:
	Xây dựng thành công mô hình học sâu có khả năng nhận diện hành vi bạo lực trong video với hiệu năng được tối ưu.
	Thiết kế và triển khai một hệ thống hoàn chỉnh, bao gồm backend xử lý video, API, và cơ sở dữ liệu, hoạt động ổn định và có khả năng mở rộng.
	Phát triển hai giao diện người dùng: một Web Dashboard cho phép quản trị viên giám sát tập trung, quản lý camera và xem lại lịch sử cảnh báo; và một ứng dụng Mobile (trên nền tảng Flutter) giúp người dùng nhận cảnh báo tức thì mọi lúc, mọi nơi.
	Đảm bảo hệ thống có khả năng xử lý đồng thời nhiều luồng video với độ trễ từ lúc phát hiện đến lúc gửi cảnh báo dưới 2 giây.
	Nội dung của luận văn được trình bày thành các chương như sau:
	Chương 1 – Tổng quan: Giới thiệu tổng quan về đề tài, các hướng tiếp cận chính, những khó khăn, thách thức và hướng giải quyết được đề xuất.
	Chương 2 – Cơ sở lý thuyết: Trình bày các kiến thức nền tảng về thị giác máy tính, học sâu, các kiến trúc mạng nơ-ron tích chập (CNN), và chi tiết về phương pháp, kiến trúc mô hình được sử dụng trong luận văn.
	Chương 3 – Xây dựng hệ thống: Mô tả chi tiết về kiến trúc hệ thống, các công nghệ được lựa chọn, quy trình thiết kế và cài đặt các thành phần từ backend đến frontend.
	Chương 4 – Thực nghiệm và Đánh giá: Trình bày về các bộ dữ liệu được sử dụng, cách thức thiết lập môi trường thực nghiệm, các kết quả đạt được về độ chính xác và hiệu năng, kèm theo các phân tích và so sánh.
	Kết luận và Hướng phát triển: Tóm tắt lại những kết quả đã đạt được, chỉ ra các hạn chế của đề tài và đề xuất các hướng nghiên cứu, phát triển trong tương lai. 
	TỔNG QUAN
GIỚI THIỆU 
	Trong kỷ nguyên số, an ninh và an toàn công cộng là một trong những mối quan tâm hàng đầu của xã hội. Các hệ thống giám sát qua video đã được triển khai rộng rãi như một công cụ thiết yếu, cung cấp khả năng quan sát và thu thập bằng chứng tại các địa điểm trọng yếu. Tuy nhiên, hiệu quả của các hệ thống này phụ thuộc lớn vào khả năng giám sát và phản ứng của con người. Thực tế cho thấy, việc một nhân viên an ninh phải theo dõi liên tục nhiều màn hình trong thời gian dài là một công việc đầy thách thức, dễ dẫn đến tình trạng mệt mỏi, mất tập trung và bỏ sót các sự kiện quan trọng, đặc biệt là các hành vi bạo lực thường bùng phát và kết thúc nhanh chóng.
	Sự tiến bộ vượt bậc của Trí tuệ Nhân tạo, đặc biệt là trong lĩnh vực Thị giác máy tính, đã mở ra một kỷ nguyên mới cho ngành an ninh giám sát. Thay vì đóng vai trò là một công cụ ghi hình thụ động, camera giờ đây có thể trở thành những "người giám sát thông minh", có khả năng tự động phân tích và hiểu được nội dung video. Khả năng phát hiện các hành vi bất thường, trong đó có bạo lực, một cách tự động và theo thời gian thực không còn là một khái niệm khoa học viễn tưởng mà đã trở thành một lĩnh vực nghiên cứu và ứng dụng đầy tiềm năng. Một hệ thống có khả năng cảnh báo ngay lập tức khi phát hiện dấu hiệu bạo lực sẽ giúp lực lượng an ninh phản ứng kịp thời, ngăn chặn leo thang xung đột, giảm thiểu thiệt hại về người và tài sản, đồng thời cung cấp bằng chứng xác thực cho quá trình điều tra sau này.
	Nhận thấy tiềm năng ứng dụng to lớn và giá trị thực tiễn mà công nghệ này mang lại, luận án đã quyết định lựa chọn và thực hiện đề tài “Ứng dụng AI phát hiện bạo lực thời gian thực từ video trên nền tảng web và mobile”. Đề tài không chỉ tập trung vào việc nghiên cứu một mô hình AI hiệu quả mà còn đặt mục tiêu xây dựng một hệ thống hoàn chỉnh, dễ dàng triển khai và sử dụng, góp phần nâng cao hiệu quả công tác giám sát an ninh trong bối cảnh hiện nay.
HƯỚNG TIẾP CẬN
	Bài toán phát hiện hành vi bạo lực trong video là một nhánh chuyên sâu của lĩnh vực Nhận dạng hành động con người. Trong những năm qua, cộng đồng nghiên cứu đã đề xuất nhiều hướng tiếp cận khác nhau, có thể được phân loại thành hai nhóm chính: các phương pháp dựa trên đặc trưng thủ công và các phương pháp dựa trên học sâu.
	Các phương pháp dựa trên đặc trưng thủ công: Trước khi học sâu trở nên phổ biến, các nhà nghiên cứu tập trung vào việc thiết kế các bộ trích xuất đặc trưng thủ công để mô tả chuyển động và ngoại hình trong video. Các phương pháp tiêu biểu bao gồm SIFT (Scale-Invariant Feature Transform), SURF (Speeded Up Robust Features) kết hợp với các kỹ thuật mô tả luồng quang học (Optical Flow) như MoSIFT (Motion SIFT). Các đặc trưng này sau đó được đưa vào những mô hình máy học truyền thống như SVM (Support Vector Machine) hoặc Random Forest để phân loại. Mặc dù đặt nền móng cho lĩnh vực, các phương pháp này bộc lộ nhiều hạn chế: chúng đòi hỏi kiến thức chuyên môn sâu để thiết kế đặc trưng, nhạy cảm với sự thay đổi của môi trường (ánh sáng, góc quay), và khó có khả năng tổng quát hóa trên các bộ dữ liệu lớn và đa dạng.
	Các phương pháp dựa trên học sâu: Sự thành công của mạng nơ-ron tích chập (CNN) trong bài toán nhận dạng hình ảnh đã thúc đẩy một cuộc cách mạng trong lĩnh vực phân tích video. Các hướng tiếp cận dựa trên học sâu cho phép mô hình tự động học các đặc trưng phức tạp trực tiếp từ dữ liệu, mang lại hiệu quả vượt trội so với các phương pháp truyền thống. Một số kiến trúc tiêu biểu bao gồm:
	Kiến trúc Hai luồng (Two-Stream Networks): Hướng tiếp cận này do Simonyan và Zisserman đề xuất, sử dụng hai mạng CNN riêng biệt. Một luồng (Spatial Stream) xử lý các khung hình tĩnh để nhận dạng đối tượng và bối cảnh. Luồng còn lại (Temporal Stream) lấy đầu vào là chuỗi các luồng quang học để học các đặc trưng về chuyển động. Kết quả của hai luồng sau đó được kết hợp để đưa ra dự đoán cuối cùng. Hướng đi này cho kết quả tốt nhưng đòi hỏi phải tính toán trước luồng quang học, một quá trình khá tốn kém về mặt tài nguyên.
	Kiến trúc CNN 3D (3D-CNN): Để nắm bắt đồng thời cả thông tin không gian và thời gian, các kiến trúc như C3D (Convolutional 3D) và I3D (Inflated 3D) đã được phát triển. Thay vì sử dụng các bộ lọc 2D trên từng khung hình, 3D-CNN sử dụng các bộ lọc 3D trượt trên một chuỗi các khung hình (video cube), cho phép mô hình học trực tiếp các đặc trưng không gian-thời gian. Mặc dù có hiệu quả cao trong việc nhận dạng hành động, các mô hình 3D-CNN thường có số lượng tham số khổng lồ, đòi hỏi năng lực tính toán rất lớn và khó đáp ứng yêu cầu xử lý thời gian thực.
	Kiến trúc dựa trên Transformer: Gần đây, các kiến trúc Transformer, vốn rất thành công trong lĩnh vực xử lý ngôn ngữ tự nhiên, đã được điều chỉnh cho các bài toán thị giác máy tính, tiêu biểu là Vision Transformer (ViT) hay TimeSformer. Các mô hình này chia video thành các khối nhỏ (patches) và sử dụng cơ chế tự chú ý (self-attention) để nắm bắt các mối quan hệ phụ thuộc ở cả tầm ngắn và tầm xa trong không gian và thời gian. Hướng tiếp cận này rất hứa hẹn nhưng cũng đi kèm với chi phí tính toán và yêu cầu dữ liệu huấn luyện cực lớn.
	Nhận thấy rằng các phương pháp hiện đại tuy mạnh mẽ nhưng thường đánh đổi bằng chi phí tính toán cao, khó triển khai trong các hệ thống giám sát thực tế với tài nguyên hạn chế, đề tài này lựa chọn một hướng tiếp cận cân bằng hơn. Cụ thể, nhóm tác giả kế thừa ý tưởng của kiến trúc hai giai đoạn (two-stage): giai đoạn đầu xác định vùng không gian có khả năng chứa hành động và giai đoạn sau phân loại hành động trong vùng đó. Tuy nhiên, đề tài đề xuất những cải tiến quan trọng để tối ưu hóa hiệu năng: giai đoạn đầu sử dụng một thuật toán trích xuất chuyển động gọn nhẹ dựa trên phân tích sự khác biệt giữa các khung hình, và giai đoạn sau sử dụng một backbone 2D-CNN hiệu quả để mô phỏng khả năng học đặc trưng không gian-thời gian của 3D-CNN. Hướng đi này nhằm đạt được sự cân bằng tối ưu giữa độ chính xác và tốc độ xử lý, phù hợp với yêu cầu của một hệ thống giám sát thời gian thực.
KHÓ KHĂN VÀ THÁCH THỨC
	Việc xây dựng một hệ thống phát hiện bạo lực tự động và chính xác trong môi trường thực tế phải đối mặt với nhiều khó khăn và thách thức đáng kể, cả về mặt học thuật lẫn kỹ thuật.
	Sự đa dạng và phức tạp của hành vi bạo lực: Bạo lực không phải là một hành động đơn lẻ mà là một tập hợp các hành vi rất đa dạng, từ đấm, đá, xô đẩy đến sử dụng vũ khí. Các hành động này có thể khác nhau rất nhiều về tốc độ, biên độ chuyển động và số lượng người tham gia. Hơn nữa, ranh giới giữa một hành vi bạo lực và một hành vi tương tự nhưng vô hại (ví dụ: một cái ôm mạnh, các động tác nhảy múa nhanh, hoặc các môn thể thao đối kháng) đôi khi rất mong manh. Điều này đặt ra một thách thức lớn cho mô hình trong việc học và phân biệt các đặc trưng tinh vi giữa các lớp hành vi.
	Các yếu tố nhiễu từ môi trường giám sát thực tế: Không giống như các bộ dữ liệu được thu thập trong môi trường phòng thí nghiệm có kiểm soát, video từ camera giám sát thực tế thường chứa rất nhiều yếu tố gây nhiễu, bao gồm:
	Thay đổi ánh sáng: Chất lượng video có thể bị ảnh hưởng bởi điều kiện ánh sáng yếu, ngược sáng, hoặc thay đổi ánh sáng đột ngột (ví dụ: đèn xe rọi qua).
	Vật cản: Người hoặc đối tượng thực hiện hành vi có thể bị che khuất một phần bởi các vật thể khác như cột, cây cối, hoặc đám đông, khiến mô hình mất đi thông tin quan trọng.
	Góc quay và khoảng cách: Camera có thể được đặt ở nhiều góc độ và khoảng cách khác nhau. Các hành động trông rất khác nhau từ góc nhìn chính diện so với từ trên cao xuống và các chi tiết sẽ mất đi khi đối tượng ở xa camera.
	Chất lượng video thấp: Nhiều hệ thống CCTV vẫn sử dụng camera có độ phân giải thấp, cùng với việc nén video làm giảm chất lượng hình ảnh, gây khó khăn cho việc trích xuất các đặc trưng chính xác.
	Yêu cầu nghiêm ngặt về xử lý thời gian thực: Đối với một hệ thống cảnh báo an ninh, tốc độ là yếu tố sống còn. Một cảnh báo chỉ có giá trị khi nó được đưa ra gần như tức thì sau khi sự kiện xảy ra. Điều này đòi hỏi toàn bộ quy trình từ việc nhận luồng video, tiền xử lý, đưa qua mô hình AI để suy luận, cho đến gửi thông báo phải diễn ra trong một khoảng thời gian ngắn (thường là dưới vài giây). Yêu cầu này tạo ra một sự đánh đổi lớn giữa độ chính xác của mô hình và tốc độ suy luận. Các mô hình càng phức tạp, càng chính xác thì thường càng chậm và ngược lại.
	Hạn chế về tài nguyên tính toán: Các hệ thống giám sát thường phải xử lý đồng thời nhiều luồng video từ hàng chục camera. Việc trang bị các máy chủ với GPU (Bộ xử lý đồ họa) cao cấp cho từng camera là không khả thi về mặt chi phí. Do đó, một thách thức lớn là phải xây dựng được một mô hình vừa đủ chính xác, vừa đủ gọn nhẹ để có thể hoạt động hiệu quả trên các phần cứng có tài nguyên hạn chế, hoặc cho phép một máy chủ duy nhất xử lý được nhiều luồng video cùng lúc.
	Giải quyết được các thách thức trên là mục tiêu cốt lõi mà đề tài hướng tới, đòi hỏi một phương pháp tiếp cận không chỉ hiệu quả về mặt thuật toán mà còn tối ưu về mặt kỹ thuật triển khai.
HƯỚNG GIẢI QUYẾT
	Để giải quyết một cách hiệu quả các khó khăn và thách thức đã được phân tích, luận văn lựa chọn một hướng tiếp cận kết hợp giữa việc áp dụng một kiến trúc AI tối ưu cho nhận dạng bạo lực và xây dựng một nền tảng phần mềm hoàn chỉnh, đáp ứng yêu cầu xử lý thời gian thực. Hướng giải quyết tổng thế bao gồm các thành phần chính sau:
Áp dụng kiến trúc AI ba module chuyên biệt
Dựa vào mô hình đã được đề xuất trong nghiên cứu của Herwin Alayn Huillcen Baca, Flor de Luz Palomino Valdivia và Juan Carlos Gutierrez Caceres trong bài báo Efficient Human Violence Recognition for Surveillance in Real Time (2024), luận văn áp dụng kiến trúc gồm ba module xử lý tuần tự nhằm cân bằng giữa độ chính xác và hiệu năng trong bối cảnh giám sát video thời gian thực. Ba module này gồm:
	Module Trích xuất Chuyển Động Không Gian (Spatial Motion Extractor - SME): Module này chịu trách nhiệm xác định các vùng chuyển động nổi bật giữa các khung hình liên tiếp, giúp loại bỏ các thông tin nền tĩnh và tập trung vào các hành động quan trọng. SME đóng vai trò tiền xử lý quan trọng để giảm nhiễu và tối ưu dữ liệu đầu vào cho các bước trích xuất đặc trưng tiếp theo.
	Module Trích xuất Đặc trưng Thời gian Ngắn (Short Temporal Extractor - STE): Để giải quyết sự phức tạp của hành vi, module này có nhiệm vụ mã hóa thông tin không gian-thời gian từ chuỗi các vùng chuyển động đã được SME trích xuất. Một backbone 2D-CNN gọn nhẹ và hiệu quả (MobileNet) được sử dụng để học các đặc trưng cấp cao. Cách tiếp cận này mô phỏng khả năng của 3D-CNN nhưng với số lượng tham số ít hơn đáng kể, trực tiếp giải quyết bài toán cân bằng giữa độ chính xác và tốc độ suy luận.
	Module Trích xuất Đặc trưng Thời gian Toàn cục (Global Temporal Extractor - GTE): GTE phân tích chuỗi đặc trưng đã được mã hóa nhằm nắm bắt các mối quan hệ thời gian dài hơn và đưa ra quyết định phân loại cuối cùng (“Bạo lực” hoặc “Không bạo lực”). Module này đảm bảo mô hình hiểu được ngữ cảnh diễn biến hành động trong toàn bộ đoạn video.
Xây dựng hệ thống dưới dạng kiến trúc Module hoá:
	Để đáp ứng yêu cầu xử lý đồng thời nhiều luồng video và đảm bảo khả năng mở rộng, hệ thống backend được thiết kế theo kiến trúc Module hóa (Modular Architecture). Mỗi thành phần chức năng (ví dụ: nhận video, xử lý AI, quản lý cảnh báo, cơ sở dữ liệu) được đóng gói thành một module độc lập. Điều này không chỉ giúp hệ thống hoạt động ổn định, dễ bảo trì và nâng cấp, mà còn cho phép tối ưu và phân bổ tài nguyên một cách linh hoạt.
Phát triển giao diện người dùng đa nền tảng:
	Để tối đa hóa tính ứng dụng và khả năng truy cập, đề tài xây dựng hai giao diện người dùng:
	Web Dashboard: Dành cho quản trị viên, cung cấp một giao diện trực quan để giám sát tập trung tất cả các luồng camera, quản lý thiết bị và xem lại lịch sử các sự kiện đã được ghi nhận.
	Ứng dụng Mobile: Cho phép người dùng cuối (nhân viên an ninh, người quản lý) nhận cảnh báo đẩy tức thì trên điện thoại di động ngay khi hệ thống phát hiện sự cố, đảm bảo phản ứng nhanh chóng bất kể vị trí.
	Bằng cách kết hợp một mô hình AI được tối ưu hóa với một kiến trúc hệ thống phần mềm hiện đại và linh hoạt, đề tài hướng đến việc cung cấp một giải pháp toàn diện và thực tiễn cho bài toán phát hiện bạo lực thời gian thực.
1.5	TẦM NHÌN: TỪ PHÁT HIỆN SỰ CỐ ĐẾN CHỦ ĐỘNG PHÒNG NGỪA
	Các hệ thống giám sát video hiện nay, dù có tích hợp AI, phần lớn vẫn dừng lại ở mức độ "phản ứng" (Reactive). Tức là, hệ thống chỉ phát huy tác dụng khi sự việc đã hoặc đang xảy ra. Dữ liệu video sau đó chủ yếu phục vụ cho công tác điều tra, mang tính chất "pháp y kỹ thuật số" (Digital Forensics) hơn là ngăn chặn.
	Nhận thấy hạn chế đó, khóa luận này đề xuất một hướng đi mở rộng: tích hợp Module Phân tích (Analytics Engine) để chuyển đổi mô hình giám sát sang trạng thái "chủ động" (Proactive). Không chỉ phát hiện sự vụ đang diễn ra, hệ thống sẽ khai thác dữ liệu lịch sử để tìm ra các quy luật hoạt động, nhận diện các điểm nóng (Hotspots) về an ninh theo thời gian và không gian. Điều này giúp lực lượng chức năng có thể bố trí nguồn lực phòng ngừa trước, ngăn chặn nguy cơ bạo lực ngay từ khi chưa hình thành. Đây chính là giá trị gia tăng cốt lõi mà đề tài mong muốn mang lại, đưa công nghệ AI thực sự trở thành một trợ lý an ninh đắc lực chứ không chỉ là một công cụ ghi hình thụ động.

CƠ SỞ LÝ THUYẾT
TỔNG QUAN VỀ THỊ GIÁC MÁY TÍNH TRONG BÀI TOÁN NHẬN DẠNG HÀNH ĐỘNG
Giới thiệu về Thị giác máy tính
	Thị giác máy tính là một lĩnh vực liên ngành của khoa học máy tính và trí tuệ nhân tạo, tập trung vào việc xây dựng các hệ thống thông minh có khả năng "nhìn" và "hiểu" thế giới thông qua dữ liệu hình ảnh và video, tương tự như cách hệ thống thị giác của con người hoạt động. Mục tiêu cốt lõi của thị giác máy tính không chỉ dừng lại ở việc thu nhận và lưu trữ hình ảnh, mà là tự động hóa các tác vụ phân tích, trích xuất thông tin, và đưa ra quyết định dựa trên nội dung trực quan. Dữ liệu đầu vào của một hệ thống thị giác máy tính thường là hình ảnh kỹ thuật số, video, hoặc các luồng hình ảnh đa chiều từ cảm biến, và đầu ra là các thông tin có ý nghĩa, ví dụ như một quyết định phân loại, một đoạn mô tả, hoặc một tập hợp các đối tượng đã được định vị.
	Lĩnh vực này bao gồm nhiều bài toán con cơ bản, tạo thành nền tảng cho các ứng dụng phức tạp hơn:
	Phân loại ảnh: Gán một nhãn duy nhất cho toàn bộ bức ảnh (ví dụ: "ảnh này chứa một con mèo").
	Phát hiện đối tượng: Không chỉ phân loại mà còn xác định vị trí của các đối tượng trong ảnh bằng cách vẽ một hộp giới hạn (bounding box) xung quanh chúng.
	Phân đoạn ngữ nghĩa: Phân loại từng pixel trong ảnh thuộc về một lớp đối tượng cụ thể (ví dụ: tô màu tất cả các pixel thuộc về "ô tô" bằng màu xanh, "người đi bộ" bằng màu đỏ).
 
Hình 2.1: Minh họa bài toán Phát hiện đối tượng trong Thị giác máy tính
Bài toán Nhận dạng hành động con người
	Trong khi các bài toán trên chủ yếu xử lý thông tin không gian trong các hình ảnh tĩnh thì nhận dạng hành động con người lại là một bài toán phức tạp hơn, đòi hỏi phải phân tích đồng thời cả thông tin không gian và thông tin thời gian. HAR tập trung vào việc xác định và phân loại các hành động mà con người thực hiện trong một chuỗi các khung hình video. Ví dụ, thay vì chỉ nhận diện "một người" và "một quả bóng", hệ thống HAR phải hiểu được hành động "người đó đang đá quả bóng".
	HAR là nền tảng công nghệ cho vô số ứng dụng thực tiễn, từ các hệ thống tương tác người-máy, phân tích hiệu suất vận động viên trong thể thao, giám sát và chăm sóc người cao tuổi, cho đến lĩnh vực trọng tâm của luận văn này: giám sát an ninh. Trong bối cảnh giám sát, bài toán phát hiện hành vi bạo lực chính là một trường hợp ứng dụng chuyên biệt của HAR, nơi mục tiêu là phân loại các chuỗi hành động thành hai lớp chính: "bạo lực" và "không bạo lực".
	Tuy nhiên, HAR là một bài toán đầy thách thức do các yếu tố như: sự thay đổi về góc nhìn, sự đa dạng trong cách thực hiện cùng một hành động của những người khác nhau, và sự tương đồng về mặt hình ảnh giữa các hành động khác nhau. Ví dụ, hành động "vỗ tay" và "tát" có thể có những chuyển động tương tự nhau ở một số giai đoạn, đòi hỏi mô hình phải nắm bắt được các đặc trưng tinh vi để phân biệt như hình 2. Do đó, việc xây dựng một mô hình HAR hiệu quả, đặc biệt cho bài toán nhạy cảm như phát hiện bạo lực, đòi hỏi các kỹ thuật và kiến trúc mô hình tiên tiến có khả năng học được các đặc trưng không gian-thời gian phức tạp.
 
Hình 2.2: Minh hoạ bài toán Nhận dạng hành động con người (HAR)
MẠNG NƠ-RON TÍCH CHẬP (CONVOLUTIONAL NEURAL NETWORK - CNN)
Giới thiệu
	Mạng Nơ-ron Tích chập, hay Convolutional Neural Network (CNN), là một lớp kiến trúc học sâu chuyên biệt, được thiết kế để xử lý và phân tích dữ liệu có cấu trúc dạng lưới (grid-like), điển hình là hình ảnh kỹ thuật số. Trước khi CNN ra đời, các mạng nơ-ron truyền thống như Mạng Perceptron Đa lớp (Multilayer Perceptron - MLP) gặp phải hai vấn đề lớn khi xử lý ảnh: (1) Bùng nổ tham số: Khi một ảnh được duỗi thành một vector đầu vào, số lượng kết nối và trọng số trong mạng trở nên khổng lồ, khiến việc huấn luyện cực kỳ tốn kém và dễ bị quá khớp (overfitting). (2) Mất thông tin không gian: Việc duỗi ảnh thành vector đã phá vỡ hoàn toàn các mối quan hệ không gian giữa các pixel lân cận, vốn là thông tin cực kỳ quan trọng để nhận dạng các đối tượng.
	Lấy cảm hứng từ cơ chế hoạt động của vỏ não thị giác ở động vật, CNN ra đời để giải quyết triệt để các vấn đề trên bằng hai ý tưởng cốt lõi: sự chia sẻ tham số (parameter sharing) và tính kết nối cục bộ (local connectivity). Thay vì mỗi nơ-ron đầu vào kết nối với tất cả các nơ-ron ở lớp tiếp theo, CNN sử dụng các bộ lọc (filter) nhỏ trượt trên toàn bộ ảnh, cho phép mô hình học các đặc trưng một cách phân cấp, từ các chi tiết đơn giản như cạnh, góc, màu sắc ở các lớp đầu, đến các cấu trúc phức tạp hơn như mắt, mũi, và cuối cùng là toàn bộ vật thể ở các lớp sau.
Các thành phần chính của Mạng CNN
	Một kiến trúc CNN điển hình được cấu thành từ việc xếp chồng ba loại lớp chính: Lớp Tích chập, Lớp Gộp, và Lớp Kết nối đầy đủ.
Lớp Tích chập (Convolutional Layer)
	Đây là thành phần cốt lõi và mang tính định danh của CNN. Nhiệm vụ của lớp này là trích xuất các đặc trưng từ ảnh đầu vào. Quá trình này được thực hiện thông qua một phép toán gọi là phép tích chập. 
	Bộ lọc (Filter/Kernel): Mỗi lớp tích chập chứa một tập hợp các bộ lọc. Một bộ lọc là một ma trận trọng số nhỏ (ví dụ: 3x3, 5x5 pixels). Mỗi bộ lọc được thiết kế để "dò tìm" một loại đặc trưng cụ thể, ví dụ một bộ lọc có thể chuyên tìm các cạnh dọc, một bộ lọc khác tìm các cạnh ngang, một bộ lọc khác tìm các mảng màu xanh lá. Các trọng số trong bộ lọc này chính là những gì mô hình sẽ học được trong quá trình huấn luyện.
	Phép tích chập: Bộ lọc sẽ trượt trên toàn bộ ảnh đầu vào, từ trái qua phải, từ trên xuống dưới. Tại mỗi vị trí, một phép nhân theo từng phần tử (element-wise multiplication) sẽ được thực hiện giữa các giá trị của bộ lọc và vùng ảnh mà nó đang che phủ, sau đó tất cả các kết quả được cộng lại để tạo ra một giá trị duy nhất.
	Bản đồ đặc trưng (Feature Map): Tập hợp tất cả các giá trị thu được từ việc trượt một bộ lọc trên toàn bộ ảnh sẽ tạo thành một ma trận hai chiều mới gọi là bản đồ đặc trưng. Bản đồ này biểu diễn "mức độ kích hoạt" của đặc trưng mà bộ lọc đó tìm kiếm tại mỗi vị trí trên ảnh. Ví dụ, nếu bộ lọc chuyên tìm cạnh dọc, các giá trị trong bản đồ đặc trưng sẽ cao ở những vùng có cạnh dọc và thấp ở những vùng khác.
	Một lớp tích chập thường chứa nhiều bộ lọc, mỗi bộ lọc tạo ra một bản đồ đặc trưng riêng, giúp mô hình học được một bộ các đặc trưng đa dạng.
 
Hình 2.3: Minh họa chi tiết phép toán tích chập 3D
Hàm kích hoạt (Activation Function)
	Ngay sau mỗi lớp tích chập, một hàm kích hoạt phi tuyến thường được áp dụng. Hàm kích hoạt phổ biến nhất trong các mạng CNN hiện đại là ReLU (Rectified Linear Unit). Hàm ReLU thực hiện một phép biến đổi rất đơn giản: nó sẽ giữ nguyên các giá trị dương và chuyển tất cả các giá trị âm thành 0 (f(x) = max(0, x)). Việc thêm vào yếu tố phi tuyến này là cực kỳ quan trọng, cho phép mạng học được các mối quan hệ phức tạp và phi tuyến tính trong dữ liệu, điều mà các phép toán tuyến tính như tích chập không thể làm được.
 
Hình 2.4: Đồ thị hàm kích hoạt ReLU
Lớp Gộp (Pooling Layer)	
	 
Hình 2.5: Sơ đồ luồng xử lý cơ bản của một mạng CNN
	Lớp gộp thường được đặt xen kẽ giữa các lớp tích chập. Mục tiêu chính của nó là giảm kích thước không gian (chiều rộng và chiều cao) của các bản đồ đặc trưng, từ đó:
	Giảm số lượng tham số và chi phí tính toán ở các lớp sau.
	Giúp mô hình trở nên bất biến với các phép dịch chuyển nhỏ (translation invariance). Điều này có nghĩa là nếu đối tượng dịch đi một vài pixel, đầu ra của lớp gộp sẽ không thay đổi nhiều, giúp mô hình trở nên mạnh mẽ hơn.
	Loại pooling phổ biến nhất là Max Pooling. Nó hoạt động bằng cách chia bản đồ đặc trưng thành các ô lưới nhỏ (ví dụ 2x2), và tại mỗi ô, nó chỉ giữ lại giá trị lớn nhất và loại bỏ các giá trị còn lại.
Lớp Kết nối đầy đủ (Fully Connected Layer)
	Sau khi đi qua nhiều lớp tích chập và lớp gộp để trích xuất các đặc trưng cấp cao, các bản đồ đặc trưng cuối cùng sẽ được "làm phẳng" (flatten) thành một vector một chiều. Vector này sau đó được đưa vào một hoặc nhiều lớp kết nối đầy đủ, tương tự như trong một mạng MLP truyền thống. Tại đây, mỗi nơ-ron được kết nối với tất cả các nơ-ron của lớp trước đó. Nhiệm vụ của các lớp này là tổng hợp các đặc trưng cấp cao đã được học và thực hiện nhiệm vụ phân loại cuối cùng, đưa ra xác suất cho mỗi lớp đối tượng.
CÁC KIẾN TRÚC CNN CHO XỬ LÝ VIDEO
	Mặc dù Mạng CNN được thiết kế nguyên bản cho hình ảnh tĩnh, các nhà nghiên cứu đã phát triển nhiều phương pháp để mở rộng năng lực của chúng sang miền video, vốn chứa đựng chiều thông tin thời gian (temporal) quý giá. Các phương pháp này có thể được chia thành các hướng tiếp cận chính sau:
Sử dụng Mạng 2D-CNN cho Video
	Hướng tiếp cận đơn giản nhất là tận dụng các kiến trúc 2D-CNN mạnh mẽ đã được huấn luyện trước trên các bộ dữ liệu ảnh khổng lồ như ImageNet.
	Phương pháp xử lý theo từng khung hình (Frame-wise processing): Trong phương pháp này, mỗi khung hình của video được xem như một hình ảnh độc lập và được đưa qua một mạng 2D-CNN để trích xuất đặc trưng hoặc đưa ra dự đoán. Kết quả từ các khung hình sau đó được tổng hợp lại (ví dụ, bằng cách lấy trung bình hoặc bỏ phiếu) để đưa ra quyết định cuối cùng cho toàn bộ video. Mặc dù có ưu điểm là tốc độ nhanh và khả năng tận dụng các mô hình có sẵn, phương pháp này có một nhược điểm chí mạng: nó hoàn toàn bỏ qua thông tin về chuyển động và mối quan hệ thời gian giữa các khung hình. Một hành động bạo lực và một hành động bình thường có thể chứa các khung hình tĩnh trông tương tự nhau, và chỉ có sự thay đổi theo thời gian mới giúp phân biệt chúng.
	Kết hợp với các mô hình tuần tự: Một cải tiến hơn là sử dụng 2D-CNN để trích xuất vector đặc trưng cho mỗi khung hình, sau đó đưa chuỗi các vector này vào một mô hình học tuần tự như Mạng Hồi quy (Recurrent Neural Network - RNN) hoặc LSTM (Long Short-Term Memory) để học các phụ thuộc thời gian. Hướng đi này hiệu quả hơn nhưng làm tăng độ phức tạp của hệ thống.
Mạng 3D-CNN (3D Convolutional Neural Networks)
	Để giải quyết trực tiếp vấn đề học đặc trưng không gian-thời gian, Mạng 3D-CNN đã được đề xuất. Thay vì sử dụng các bộ lọc 2D (chiều cao x chiều rộng) trượt trên từng khung hình, 3D-CNN sử dụng các bộ lọc 3D (thời gian x chiều cao x chiều rộng).
 
Hình 2.6: So sánh giữa phép tích chập 2D và 3D
	Nguyên lý hoạt động: Một bộ lọc 3D, ví dụ kích thước 3x3x3, sẽ thực hiện phép tích chập trên một "khối video" gồm 3 khung hình liên tiếp. Bằng cách này, bộ lọc có thể đồng thời nắm bắt được các đặc trưng về ngoại hình trong không gian (như hình dạng, kết cấu) và các đặc trưng về chuyển động trong thời gian (như một vật thể di chuyển từ trái sang phải). Điều này cho phép mô hình học được các biểu diễn không gian-thời gian (spatio-temporal features) một cách tự nhiên và hiệu quả.
	Ưu và nhược điểm: Các kiến trúc 3D-CNN tiêu biểu như C3D và I3D đã đạt được kết quả ấn tượng trên nhiều bộ dữ liệu nhận dạng hành động. Tuy nhiên, ưu điểm về hiệu quả học thuật lại đi kèm với một chi phí tính toán khổng lồ. Số lượng tham số trong mạng 3D-CNN lớn hơn rất nhiều so với mạng 2D-CNN tương đương, đòi hỏi tài nguyên phần cứng (GPU, bộ nhớ) mạnh mẽ và một lượng lớn dữ liệu video để huấn luyện nhằm tránh hiện tượng quá khớp. Do đó, việc triển khai các mô hình 3D-CNN trong các hệ thống yêu cầu xử lý thời gian thực với tài nguyên hạn chế là một thách thức lớn.
Giới thiệu MobileNetV2 – Kiến trúc gọn nhẹ và hiệu quả
	Nhận thấy sự đánh đổi giữa độ chính xác và hiệu năng, các kiến trúc CNN gọn nhẹ đã ra đời nhằm mục đích giảm thiểu chi phí tính toán mà vẫn duy trì được hiệu quả trích xuất đặc trưng. MobileNetV2 là một trong những kiến trúc tiêu biểu và thành công nhất trong nhóm này, và cũng là backbone được lựa chọn trong mô hình của luận văn.
	Sự hiệu quả của MobileNetV2 đến từ việc sử dụng một khối xây dựng chính gọi là Tích chập tách biệt theo chiều sâu (Depthwise Separable Convolution). Thay vì thực hiện một phép tích chập tiêu chuẩn, kiến trúc này tách nó thành hai bước:
	Tích chập theo chiều sâu (Depthwise Convolution): Áp dụng một bộ lọc 2D riêng biệt cho từng kênh đầu vào (input channel). Giai đoạn này chỉ thực hiện lọc không gian mà không kết hợp thông tin giữa các kênh.
	Tích chập điểm (Pointwise Convolution): Sử dụng các bộ lọc 1x1 để kết hợp thông tin từ các kênh đầu ra của bước trên. Giai đoạn này thực hiện việc kết hợp đặc trưng giữa các kênh.
 
Hình 2.7: Minh họa kiến trúc Tích chập Tách biệt theo Chiều sâu (Depthwise Separable Convolution)
	Bằng cách tách phép tích chập tiêu chuẩn thành hai bước riêng biệt, MobileNetV2 giảm đáng kể số lượng phép tính và tham số so với các mạng CNN truyền thống, trong khi vẫn giữ được khả năng học đặc trưng mạnh mẽ. Ngoài ra, kiến trúc này còn giới thiệu các khối "nút cổ chai tuyến tính" (linear bottleneck) và "kết nối phần dư ngược" (inverted residual connection) để cải thiện luồng lan truyền gradient và tăng hiệu quả học tập. Với những đặc tính trên, MobileNetV2 trở thành một lựa chọn lý tưởng cho các ứng dụng thị giác máy tính trên thiết bị di động và các hệ thống yêu cầu xử lý thời gian thực, phù hợp hoàn hảo với mục tiêu của đề tài.
CƠ SỞ LÝ THUYẾT CỦA MÔ HÌNH ĐỀ XUẤT
	Để giải quyết các thách thức về hiệu năng và độ chính xác, mô hình được đề xuất trong luận văn này được lấy cảm hứng và kế thừa từ nghiên cứu của tác giả trong bài báo [3] (Sensors 2024). Kiến trúc bao gồm ba module tuần tự: Module Trích xuất Chuyển động Không gian (SME), Module Trích xuất Đặc trưng Thời gian Ngắn (STE), và Module Trích xuất Đặc trưng Thời gian Toàn cục (GTE). Mỗi module được thiết kế với một nhiệm vụ chuyên biệt, kết hợp các kỹ thuật xử lý ảnh cổ điển và các kiến trúc học sâu hiện đại một cách hiệu quả.
Module Trích xuất Chuyển động Không gian (SME)


 
Hình 2.8: Module Trích xuất Chuyển động Không gian (SME) 
	Module SME đóng vai trò là bước tiền xử lý thông minh, có mục tiêu xác định và tách biệt các Vùng quan tâm (Region of Interest - ROI) chứa chuyển động khỏi nền tĩnh trong video. Thay vì sử dụng các mạng phát hiện đối tượng nặng nề, module này dựa trên nguyên lý cơ bản rằng các hành vi bạo lực luôn bao gồm các chuyển động nhanh và rõ rệt.
	Quá trình hoạt động của SME bao gồm ba bước chính:
Tính toán khoảng cách Euclidean giữa các khung hình liên tiếp
	Để định lượng sự chuyển động, module lấy đầu vào là hai khung hình RGB liên tiếp, ký hiệu là Ft và Ft+1, có kích thước R³ˣᵂˣᴴ. Sự khác biệt giữa hai khung hình này được tính toán cho từng pixel thông qua khoảng cách Euclidean. Kết quả là một ảnh xám dₜ nơi các pixel không có chuyển động sẽ có giá trị gần bằng 0 (màu đen), trong khi các pixel có sự thay đổi lớn về màu sắc (do chuyển động) sẽ có giá trị cao hơn (màu sáng). Công thức được định nghĩa như sau:
d_t=\sqrt{\sum_{i=1}^{3}\left(F_{t+1}^i-F_t^i\right)^2}
Trong đó:
	dₜ là ảnh chênh lệch (ảnh chuyển động) tại thời điểm t.
	Fₜ và Fₜ₊₁ là hai khung hình liên tiếp.
	i đại diện cho các kênh màu (R, G, B).
 
Hình 2.9: Khoảng cách Euclidean giữa hai khung hình liên tiếp Ft, Ft+1
Áp dụng các phép toán hình thái học (Morphological Operations)
	Ảnh chuyển động dₜ thu được ở bước trên thường chứa nhiều nhiễu và các vùng chuyển động có thể bị rời rạc. Để tạo ra một vùng quan tâm liền mạch, một phép toán hình thái học gọi là phép giãn nở (Dilation) được áp dụng. Phép giãn nở hoạt động bằng cách trượt một cửa sổ nhỏ (kernel) trên ảnh và tại mỗi vị trí, pixel trung tâm sẽ được gán giá trị lớn nhất trong vùng lân cận được xác định bởi kernel. Quá trình này có tác dụng làm dày và kết nối các vùng sáng lại với nhau, tạo thành một mặt nạ (mask) nhị phân bₜ duy nhất bao phủ toàn bộ khu vực có chuyển động.
 
Hình 2.10: Phép toán hình thái học
Trích xuất Vùng quan tâm (ROI)
	Cuối cùng, để trích xuất các pixel chuyển động thực sự từ khung hình gốc, một phép nhân theo từng phần tử (dot product) được thực hiện giữa khung hình Ft+1 và mặt nạ nhị phân bₜ. Kết quả là một khung hình mới Mt, trong đó nền tĩnh đã được loại bỏ hoàn toàn (chuyển thành màu đen), chỉ giữ lại các pixel thuộc về vùng có chuyển động. Chuỗi các khung hình Mt này sẽ là đầu vào cho module tiếp theo.
 
Hình 2.11: Mt: Trích xuất không gian của chuyển động
Module Trích xuất Đặc trưng Thời gian Ngắn (STE)
	Module STE có nhiệm vụ học các đặc trưng không gian - thời gian từ chuỗi các khung hình chuyển động M do SME tạo ra. Để tránh chi phí tính toán của 3D-CNN, module này sử dụng một kỹ thuật thông minh để "mã hóa" thông tin thời gian ngắn hạn vào các kênh màu của một hình ảnh, cho phép sử dụng một backbone 2D-CNN hiệu quả là MobileNetV2.

 
Hình 2.12: Module Trích xuất Đặc trưng Thời gian Ngắn (STE)
	Quá trình hoạt động bao gồm các bước:
	Chuyển đổi sang ảnh xám: Mỗi khung hình chuyển động Mt (3 kênh màu) được chuyển thành một ảnh xám pₜ (1 kênh) bằng cách lấy trung bình giá trị của ba kênh màu. 
p_t=\frac{1}{3}\sum_{c=1}^{3}\left(M_t^c\right)
Trong đó:
	p_t là ảnh xám tại thời điểm t.
	M_t^c là giá trị pixel của kênh màu c (1: Đỏ, 2: Lục, 3: Lam) tại thời điểm t.
	Ghép kênh thời gian: Module lấy ba ảnh xám liên tiếp pₜ, pₜ₊₁, và pₜ₊₂ và ghép chúng thành một ảnh 3 kênh duy nhất P. Trong ảnh P này, kênh màu Đỏ (Red) tương ứng với pₜ, kênh Lục (Green) tương ứng với pₜ₊₁, và kênh Lam (Blue) tương ứng với pₜ₊₂. Bằng cách này, thông tin về sự thay đổi của chuyển động trong 3 khung hình liên tiếp đã được mã hóa vào một cấu trúc dữ liệu duy nhất mà 2D-CNN có thể xử lý.
	Trích xuất đặc trưng: Ảnh 3 kênh P này sau đó được đưa vào mạng MobileNetV2 để trích xuất một bản đồ đặc trưng cấp cao B.
Module Trích xuất Đặc trưng Thời gian Toàn cục (GTE)
 
Hình 2.13: Module Trích xuất Đặc trưng Thời gian Toàn cục (GTE)
	Sau khi STE đã nắm bắt các đặc trưng chuyển động trong một cửa sổ thời gian ngắn (3 khung hình), module GTE có nhiệm vụ phân tích mối quan hệ trên một chuỗi các đặc trưng dài hơn để đưa ra quyết định cuối cùng.
	Quá trình này bao gồm:
	Nén không gian (Spatial Compression): Đầu vào của GTE là một chuỗi các bản đồ đặc trưng B từ STE. Để tổng hợp thông tin không gian, một phép Gộp Trung bình Toàn cục (Global Average Pooling - GAP) được áp dụng trên mỗi kênh c của mỗi bản đồ đặc trưng. Phép toán này tính giá trị trung bình của tất cả các pixel trong kênh đó, biến mỗi bản đồ đặc trưng 2D (H x W) thành một giá trị vô hướng duy nhất. Kết quả là một vector đặc trưng S.
S^c=\frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}{B^c\left(i,j\right)}
Trong đó:
	S^c là giá trị đặc trưng trung bình của kênh c.
	H, W là chiều cao và chiều rộng của bản đồ đặc trưng.
	B^c\left(i,j\right) là giá trị tại vị trí (i,j) trên kênh của bản đồ đặc trưng.
	Nén thời gian (Temporal Compression): Vector S sau đó được nén theo chiều kênh để tạo ra một vector q duy nhất đại diện cho thông tin tại một thời điểm.
q=\frac{1}{C}\sum_{c=1}^{C}S^c
Trong đó:
	q là vector đặc trưng nén theo thời gian.
	C là tổng số kênh đặc trưng.
	S^c giá trị đặc trưng của kênh c.
	Phân loại: Chuỗi các vector q theo thời gian được đưa qua các lớp kết nối đầy đủ (Fully Connected Layers). Một cơ chế tương tự attention được sử dụng để "cân" lại trọng số của các đặc trưng quan trọng theo thời gian. Cuối cùng, một hàm kích hoạt Sigmoid được sử dụng ở lớp đầu ra để đưa ra xác suất của hành vi là "Bạo lực" (gần 1) hay "Không Bạo lực" (gần 0).
LỰA CHỌN VÀ TỐI ƯU HÓA MẠCH XƯƠNG SỐNG (BACKBONE) CHO MÔ HÌNH ĐỀ XUẤT
Trong mô hình gốc được trình bày trong bài báo [3], module STE sử dụng một mạng 2D-CNN nhẹ - MobileNetV2 làm backbone để trích xuất đặc trưng không gian và thời gian ngắn hạn. Đây là lựa chọn hợp lý vì MobileNetV2 có cấu trúc Inverted Residual và Depthwise Separable Convolution, giúp giảm đáng kể số lượng tham số và FLOPs so với các CNN truyền thống.
Tuy nhiên, còn tồn tại các backbone khác đáng cân nhắc, như MobileNetV3. MobileNetV3 tận dụng các cải tiến từ Neural Architecture Search (NAS) và Squeeze-and-Excitation (SE) block, cùng với hàm kích hoạt h-swish, giúp giảm số lượng tham số và FLOPs so với MobileNetV2, trong khi vẫn duy trì độ chính xác cao. Do đó, để tối ưu hóa hiệu năng và giảm độ trễ suy luận trên các thiết bị biên, luận văn này lựa chọn MobileNetV3-Small làm backbone cho module STE. Sự thay đổi này vừa tận dụng được các tiến bộ kỹ thuật mới, vừa đảm bảo mô hình vận hành hiệu quả trong môi trường giám sát video thời gian thực, đồng thời duy trì độ chính xác cao trong việc trích xuất đặc trưng không gian từ các khung hình chuyển động.
Bảng 2.1: Bảng so sánh Params, Accuracy và FLOPs giữa các Backbone (MobileNetV2, MobileNetV3, EfficientNet B0, MNasNet) trên RWF – 2000 và HockeyFight
Backbone	Độ chính xác (RWF-2000)	Độ chính xác (Hockey Fight)	Tham số	FLOPs
MobileNetV2	81%	98%	3.51M	3.15G
MobileNetV3 Small	82%	97.75%	2.54M	1.25G
MobileNetV3 Large	88.25%	98%	5.49M	4.70 G
EfficientNet B0	88.25%	98%	5.29M	4.17G
MNasNet	75.25%	81%	2.22M	1.13G

Từ bảng 2.1, có thể thấy MobileNetV3-Small giảm đáng kể số lượng tham số và FLOPs so với MobileNetV2 và các kiến trúc lớn hơn, trong khi vẫn duy trì độ chính xác gần tương đương trên cả hai tập dữ liệu. Điều này khẳng định rằng MobileNetV3-Small là lựa chọn phù hợp để cân bằng giữa hiệu năng và độ chính xác trong ứng dụng giám sát video thời gian thực.
CÁC CHỈ SỐ ĐÁNH GIÁ MÔ HÌNH
	Để đánh giá hiệu suất của một mô hình học máy một cách khách quan và toàn diện, việc lựa chọn các chỉ số đo lường phù hợp là cực kỳ quan trọng. Trong các bài toán phân loại nhị phân (binary classification) như phát hiện bạo lực ("Violence" vs. "Non-violence"), việc chỉ sử dụng một chỉ số duy nhất như độ chính xác (Accuracy) có thể không phản ánh đúng bản chất vấn đề, đặc biệt khi bộ dữ liệu bị mất cân bằng (imbalanced). Do đó, luận văn này sử dụng một bộ các chỉ số tiêu chuẩn để cung cấp một cái nhìn đa chiều về hiệu quả của mô hình.
	Các chỉ số này được tính toán dựa trên bốn giá trị cơ bản rút ra từ Ma trận nhầm lẫn (Confusion Matrix):
	True Positive (TP): Số trường hợp mô hình dự đoán đúng là "Bạo lực" (Violence).
	True Negative (TN): Số trường hợp mô hình dự đoán đúng là "Không Bạo lực" (Non-violence).
	False Positive (FP): Số trường hợp mô hình dự đoán nhầm là "Bạo lực" trong khi thực tế là "Không Bạo lực". Đây còn gọi là lỗi "báo động giả" (Type I Error).
	False Negative (FN): Số trường hợp mô hình dự đoán nhầm là "Không Bạo lực" trong khi thực tế là "Bạo lực". Đây là lỗi nguy hiểm nhất trong bài toán này, vì nó có nghĩa là đã bỏ sót một sự kiện cần cảnh báo (Type II Error).
	Từ bốn giá trị trên, các chỉ số đánh giá chính được định nghĩa như sau:
Độ chính xác (Accuracy)
	Đây là chỉ số tổng quan nhất, đo lường tỷ lệ phần trăm số điểm dữ liệu được mô hình phân loại đúng trên tổng số dữ liệu.
\mathrm{Accuracy}=\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{TP}+\mathrm{TN}+\mathrm{FP}+\mathrm{FN}}
Trong đó:
	TP, TN, FP, FN lần lượt là True Positive, True Negative, False Positive, False Negative.
	Tuy nhiên, nếu bộ dữ liệu có 99% là video "Không Bạo lực" và 1% là "Bạo lực", một mô hình lười biếng chỉ cần dự đoán tất cả là "Không Bạo lực" cũng đã đạt được độ chính xác 99%, nhưng lại hoàn toàn vô dụng trong thực tế. Vì vậy, chúng ta cần các chỉ số chuyên biệt hơn.
Độ chính xác dự báo (Precision)
	Precision trả lời cho câu hỏi: "Trong số tất cả các trường hợp được dự đoán là 'Bạo lực', có bao nhiêu trường hợp thực sự là 'Bạo lực'?" Chỉ số này đo lường mức độ tin cậy của các cảnh báo mà mô hình đưa ra, giúp đánh giá tỷ lệ báo động giả.
\mathrm{Precision}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}
Trong đó:
	TP là số trường hợp dự đoán đúng là Bạo lực.
	FP là số trường hợp dự đoán sai là Bạo lực (Báo động giả).
Độ thu hồi (Recall / Sensitivity)
	Recall trả lời cho câu hỏi: "Trong số tất cả các trường hợp thực sự là 'Bạo lực', mô hình đã phát hiện được bao nhiêu trường hợp?" Chỉ số này đo lường khả năng của mô hình trong việc "bắt" được các ca dương tính, giúp đánh giá tỷ lệ bỏ sót sự kiện. Trong bài toán an ninh, Recall là một chỉ số cực kỳ quan trọng.
\mathrm{Recall}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}
Trong đó:
	TP là số trường hợp dự đoán đúng là Bạo lực.
	FN là số trường hợp bỏ sót Bạo lực.
F1-Score
	Trong nhiều trường hợp, có một sự đánh đổi giữa Precision và Recall (ví dụ: một mô hình quá nhạy có thể có Recall cao nhưng Precision thấp do nhiều báo động giả). F1-Score là trung bình điều hòa (harmonic mean) của Precision và Recall, cung cấp một chỉ số duy nhất để cân bằng cả hai yếu tố này. F1-Score cao khi cả Precision và Recall đều cao.
\mathrm{F1-Score}=2\cdot\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}
Trong đó:
	Precision là độ chính xác dự báo.
	Recall là độ thu hồi.
	Việc sử dụng đồng thời bộ bốn chỉ số này sẽ cho phép luận án đưa ra những đánh giá sâu sắc và đáng tin cậy về hiệu quả của mô hình đề xuất trong chương Thực nghiệm.
2.7 CƠ SỞ LÝ THUYẾT VỀ PHÂN TÍCH DỮ LIỆU VÀ KIẾN TRÚC HỆ THỐNG
2.7.1	Học máy trong Phân tích dữ liệu (Data Analytics)
	Bên cạnh việc phát hiện hành vi bạo lực tức thời từ video, một hệ thống giám sát hiện đại cần có khả năng phân tích dữ liệu lịch sử để đưa ra các dự báo và nhận diện xu hướng, giúp chuyển đổi từ thế bị động sang chủ động phòng ngừa. Để thực hiện điều này, luận án áp dụng hai thuật toán học máy kinh điển là Random Forest (cho bài toán phân loại/dự báo rủi ro) và K-Means Clustering (cho bài toán phân cụm/nhận diện điểm đen).
2.7.2	Thuật toán Random Forest (Rừng ngẫu nhiên)
	Random Forest là một phương pháp học máy thuộc nhóm Ensemble Learning (Học kết hợp), hoạt động bằng cách xây dựng một "rừng" gồm nhiều Cây quyết định (Decision Trees) trong quá trình huấn luyện.
	Nguyên lý hoạt động: Thay vì dựa vào một cây quyết định duy nhất (dễ bị quá khớp - overfitting), Random Forest tạo ra nhiều cây con, mỗi cây được huấn luyện trên một tập con ngẫu nhiên của dữ liệu (Bagging) và sử dụng một tập con ngẫu nhiên của các đặc trưng (Feature Randomness). Kết quả dự đoán cuối cùng được quyết định bằng cơ chế lấy số đông (Voting) đối với bài toán phân loại hoặc trung bình (Averaging) đối với bài toán hồi quy.
	Lý do lựa chọn: Thuật toán này có độ chính xác cao, khả năng chống nhiễu tốt, và đặc biệt hiệu quả trong việc xử lý các dữ liệu dạng bảng có cấu trúc (như log sự kiện giám sát bao gồm thời gian, địa điểm, loại sự kiện) để dự báo mức độ rủi ro an ninh.
2.7.3	Thuật toán K-Means Clustering (Phân cụm K-Means)
	K-Means là một thuật toán học không giám sát (Unsupervised Learning) phổ biến nhất dùng để gom nhóm dữ liệu.
	Nguyên lý hoạt động: Thuật toán phân chia tập dữ liệu thành K nhóm (clusters) rời rạc sao cho khoảng cách từ các điểm dữ liệu đến tâm của nhóm (centroid) là nhỏ nhất. Quá trình này được thực hiện lặp đi lặp lại: (1) Gán mỗi điểm vào nhóm có tâm gần nhất; (2) Cập nhật lại vị trí tâm nhóm dựa trên trung bình toạ độ của các điểm trong nhóm đó.
	Lý do lựa chọn: Trong bài toán giám sát, K-Means cực kỳ hữu ích để tự động phát hiện các "Hotspots" (Điểm nóng) - tức là các nhóm sự kiện có tính chất tập trung cao về thời gian hoặc địa điểm, giúp người quản lý tối ưu hóa việc phân bổ lực lượng bảo vệ.
2.7.4	Kiến trúc hướng sự kiện (Event-Driven Architecture) với Kafka
	Trong các hệ thống xử lý video thời gian thực quy mô lớn, kiến trúc Monolith (nguyên khối) truyền thống thường gặp vấn đề nghẽn cổ chai khi lưu lượng dữ liệu tăng đột biến. Để giải quyết, mô hình Event-Driven Architecture (EDA) được áp dụng với Apache Kafka đóng vai trò là xương sống (Backbone).
	Nguyên lý: Kafka hoạt động như một hệ thống Pub/Sub (Publish/Subscribe) phân tán, có khả năng chịu lỗi cao. Dữ liệu (Message) được "sản xuất" (Produce) vào các Topic và được "tiêu thụ" (Consume) bởi nhiều dịch vụ khác nhau một cách bất đồng bộ. Điều này giúp tách rời (decouple) sự phụ thuộc giữa các thành phần hệ thống (ví dụ: module AI không cần biết module Backend hoạt động ra sao, chỉ cần lấy dữ liệu từ Kafka), đảm bảo hệ thống vận hành mượt mà và dễ dàng mở rộng.

2.8 NỀN TẢNG PHÁT TRIỂN ỨNG DỤNG DI ĐỘNG
	Để hoàn thiện hệ thống và đưa các cảnh báo an ninh đến tay người dùng cuối một cách nhanh chóng và hiệu quả, một ứng dụng di động đã được phát triển. Việc lựa chọn công nghệ cho ứng dụng này được cân nhắc kỹ lưỡng để đảm bảo các yếu tố về hiệu năng, tốc độ phát triển và khả năng mở rộng. Nền tảng được lựa chọn bao gồm framework Flutter cho phía client (giao diện người dùng) và Firebase cho phía backend (dịch vụ nền).
Flutter - Framework phát triển ứng dụng đa nền tảng
	Flutter là một bộ công cụ giao diện người dùng (UI toolkit) mã nguồn mở được phát triển bởi Google. Nó cho phép các nhà phát triển xây dựng các ứng dụng đẹp mắt, được biên dịch tự nhiên (natively compiled) cho di động (iOS, Android), web và máy tính để bàn từ một cơ sở mã nguồn duy nhất. Luận án đã lựa chọn Flutter vì những ưu điểm vượt trội sau:
	Phát triển đa nền tảng thực thụ: Flutter cho phép viết mã một lần và triển khai trên cả hai hệ điều hành phổ biến nhất là Android và iOS. Điều này giúp tiết kiệm từ 50-70% thời gian và nguồn lực so với việc phải xây dựng hai ứng dụng riêng biệt bằng ngôn ngữ gốc (Java/Kotlin cho Android và Swift cho iOS).
	Hiệu năng gần như tương đương ứng dụng gốc (Native Performance): Không giống như các framework dựa trên WebView hoặc cầu nối JavaScript (JavaScript Bridge), Flutter biên dịch mã Dart trực tiếp ra mã máy ARM hoặc x64. Nó cũng sử dụng engine đồ họa Skia của riêng mình để vẽ giao diện người dùng, cho phép đạt được tốc độ hoạt ảnh mượt mà, ổn định ở mức 60 khung hình/giây (fps).
	Tính năng Hot Reload tăng tốc phát triển: Flutter cung cấp tính năng "Hot Reload", cho phép các nhà phát triển thấy được những thay đổi trong mã nguồn gần như ngay lập tức trên ứng dụng đang chạy (thường dưới 1 giây) mà không cần phải biên dịch lại toàn bộ dự án. Điều này giúp đẩy nhanh đáng kể chu trình thử nghiệm và sửa lỗi.
	Giao diện người dùng phong phú và hệ sinh thái mạnh mẽ: Flutter cung cấp một bộ widget lớn, tuân thủ theo cả hai ngôn ngữ thiết kế Material Design (của Android) và Cupertino (của iOS). Cùng với đó là một hệ sinh thái với hơn 45,000 thư viện (packages) trên kho lưu trữ pub.dev, hỗ trợ đầy đủ các tính năng từ phát video, bản đồ, đến tích hợp các dịch vụ của Firebase.
Firebase - Nền tảng Backend-as-a-Service (BaaS)
	Firebase là một nền tảng phát triển ứng dụng của Google, cung cấp một bộ các dịch vụ backend (Backend-as-a-Service - BaaS) mạnh mẽ, giúp đơn giản hóa và tăng tốc quá trình phát triển các tính năng phức tạp. Thay vì phải tự xây dựng và quản lý máy chủ cho các tác vụ như xác thực, cơ sở dữ liệu hay thông báo đẩy, nhà phát triển có thể tích hợp các dịch vụ có sẵn của Firebase. Trong khuôn khổ của đề tài, các dịch vụ sau đã được sử dụng:
Firebase Authentication (Xác thực người dùng)
	Dịch vụ này cung cấp một hệ thống backend an toàn và dễ sử dụng để quản lý danh tính người dùng. Ứng dụng đã tích hợp hai phương thức xác thực: Email/Mật khẩu và Đăng nhập bằng tài khoản Google. Firebase Authentication xử lý toàn bộ luồng đăng nhập, đăng ký, đặt lại mật khẩu và quản lý phiên đăng nhập một cách bảo mật, giúp tiết kiệm đáng kể thời gian phát triển.
Cloud Firestore và Cloud Storage (Lưu trữ dữ liệu)
	Để đáp ứng các nhu cầu lưu trữ khác nhau, ứng dụng đã kết hợp hai dịch vụ của Firebase:
	Cloud Firestore: Được sử dụng để lưu trữ metadata của các sự kiện (events). Firestore là một cơ sở dữ liệu NoSQL, linh hoạt, lưu trữ dữ liệu dưới dạng các tài liệu JSON. Nó hỗ trợ truy vấn phức tạp và đồng bộ hóa dữ liệu thời gian thực, cho phép ứng dụng cập nhật giao diện ngay lập tức khi có sự thay đổi trên cơ sở dữ liệu. Cấu trúc một bản ghi sự kiện trong Firestore bao gồm các thông tin như eventId, cameraId, timestamp, và một đường dẫn tham chiếu đến file video.
	Cloud Storage for Firebase: Được sử dụng để lưu trữ các file video và ảnh thumbnail của các sự kiện. Đây là một dịch vụ lưu trữ đối tượng (object storage) mạnh mẽ, được thiết kế để lưu trữ các tệp tin nhị phân có dung lượng lớn. Việc lưu video trên Cloud Storage mang lại nhiều lợi ích: khả năng mở rộng gần như vô hạn, tích hợp mạng phân phối nội dung (CDN) để tăng tốc độ tải trên toàn cầu, và hệ thống quy tắc bảo mật (Security Rules) để kiểm soát quyền truy cập chặt chẽ.
Firebase Cloud Messaging - FCM (Cảnh báo thời gian thực)
	FCM là dịch vụ cốt lõi cho tính năng cảnh báo tức thì. Khi hệ thống AI ở backend phát hiện một hành vi bạo lực, nó sẽ không chờ client phải chủ động hỏi (polling) mà sẽ sử dụng FCM để gửi một thông báo đẩy (push notification) trực tiếp đến các thiết bị di động của người dùng được phân quyền. Luồng hoạt động này cực kỳ hiệu quả, giúp tiết kiệm pin cho thiết bị, giảm tải cho mạng và đảm bảo thông báo được gửi đi với độ trễ dưới 1 giây.
Chức năng Báo cáo (Report) sử dụng Firestore Sub-collection
	Khi người dùng cho rằng một cảnh báo là không chính xác, họ có thể báo cáo lại. Để quản lý các báo cáo này một cách có tổ chức và hiệu quả, ứng dụng sử dụng cấu trúc sub-collection của Firestore. Mỗi báo cáo sẽ được lưu thành một tài liệu riêng trong một sub-collection reports nằm bên trong tài liệu event tương ứng. Cách tiếp cận này giúp dữ liệu được tổ chức phân cấp, dễ dàng truy vấn tất cả các báo cáo của một sự kiện mà không làm tài liệu event chính bị phình to, tránh vượt qua giới hạn kích thước 1MB của Firestore.
Thư viện phát video: video_player
	Để hiển thị các video sự kiện được lưu trữ trên Cloud Storage, ứng dụng đã sử dụng video_player, một thư viện chính thức do đội ngũ Flutter phát triển. Thư viện này được lựa chọn vì tính ổn định, gọn nhẹ và cung cấp đầy đủ các chức năng cơ bản (phát, tạm dừng, tua video) cần thiết cho các trường hợp sử dụng đơn giản của ứng dụng.
 
XÂY DỰNG THUẬT TOÁN VÀ ỨNG DỤNG
TỔNG QUAN VỀ LUỒNG XỬ LÝ (PIPELINE)
	Hệ thống được thiết kế để hoạt động trong thời gian thực, xử lý liên tục các luồng video từ camera giám sát để phát hiện các hành vi bất thường. Luồng xử lý dữ liệu (Data Pipeline) được chia thành các giai đoạn tuần tự, đảm bảo tính chính xác và hiệu quả tính toán.
 
Hình 3.1: Sơ đồ luồng xử lý tổng quát của hệ thống
	Hệ thống phát hiện bạo lực được xây dựng theo kiến trúc hướng sự kiện (Event-Driven Architecture) với Apache Kafka làm trung tâm, đảm bảo khả năng xử lý song song và mở rộng dễ dàng. Quy trình xử lý dữ liệu (Data Pipeline) được chia thành 3 giai đoạn chính:
	1. Thu thập dữ liệu (Ingestion): Các Camera Worker kết nối với RTSP Server, đọc liên tục luồng video, chia nhỏ thành các frame và đẩy vào hàng đợi thông điệp (Message Queue) trên Kafka.
	2. Xử lý thông minh (AI Processing): AI Consumer Service - hoạt động như một đơn vị xử lý độc lập - liên tục lấy dữ liệu từ Kafka. Tại đây, hệ thống duy trì bộ đệm (Buffer) cục bộ gồm 30 khung hình để tái tạo ngữ cảnh thời gian, sau đó đưa qua mô hình học sâu (Deep Learning) để phân tích hành vi.
	3. Cảnh báo và Lưu trữ: Kết quả phân tích (Bạo lực/Không bạo lực) được gửi song song đến hai đích: (a) Redis Pub/Sub để phát cảnh báo tức thời tới ứng dụng Web/Mobile; (b) HDFS để lưu trữ dài hạn phục vụ phân tích xu hướng và tái huấn luyện mô hình.
	Cách tổ chức này thay thế hoàn toàn mô hình xử lý tuần tự truyền thống, giúp hệ thống không bị nghẽn cổ chai khi số lượng camera tăng lên.	
BỘ DỮ LIỆU THỰC NGHIỆM
	Dữ liệu huấn luyện là yếu tố then chốt quyết định chất lượng của mô hình học sâu. Trong nghiên cứu này, hệ thống được huấn luyện và đánh giá trên hai bộ dữ liệu chuẩn trong lĩnh vực phát hiện bạo lực: RWF-2000 và Hockey Fight Dataset.
Tổng quan về bộ dữ liệu
	Việc sử dụng hai bộ dữ liệu khác nhau giúp đánh giá khả năng tổng quát hóa của mô hình trên các ngữ cảnh và điều kiện quay khác nhau. RWF-2000 cung cấp dữ liệu đa dạng từ nhiều nguồn thực tế, trong khi Hockey Fight tập trung vào môi trường thể thao với điều kiện ánh sáng và góc quay tương đối đồng nhất.
RWF-2000 Dataset
	Nguồn gốc: Công bố bởi Ming Cheng và cộng sự (2021).
	Đặc điểm: 2000 video clips (1000 bạo lực, 1000 không bạo lực), độ dài trung bình 5s, 30 FPS.
	Nguồn thu thập: CCTV, Dashcam, Mobile camera.
	Thách thức: Chất lượng không đồng nhất, ánh sáng yếu, góc quay đa dạng.
 
Hình 3.2: Một số mẫu khung hình từ tập dữ liệu RWF-2000


 
Hình 3.3: Biểu đồ phân bố độ dài video RWF-2000
Hockey Fight Dataset
	Nguồn gốc: Nievas và cộng sự (2011).
	Đặc điểm: 1000 video clips (500 bạo lực, 500 không bạo lực), độ dài 2s, 25 FPS.
	Bối cảnh: Sân thi đấu khúc côn cầu.
	Thách thức: Tốc độ chuyển động cao, nền trắng (sân băng), nhiều người trong khung hình.
	Điểm đặc biệt của bộ dữ liệu này là luôn luôn có người xuất hiện trong tất cả các khung hình ở mọi thời điểm, đồng thời nền trắng phía sau không bị nhiễu, ít xuất hiện vật thể lạ (điều mà các bộ dữ liệu khác thường ít khi làm được), giúp cho người ở trong các khung hình nổi bật hơn hẳn.
 
Hình 3.4: Một số mẫu khung hình từ tập dữ liệu Hockey Fight
 
Hình 3.5: Các chỉ số đặc trưng của hành vi bạo lực
Real Life Violence Situations (RLVS) Dataset
	Nguồn: Bộ dữ liệu được công bố bởi Mohamed Mostafa Mustafa trên Kaggle [6], thu thập từ các nguồn video thực tế đa dạng trên internet.
 
Hình 3.6: Một số mẫu khung hình từ tập dữ liệu RLVS
	Thống kê:
	Tổng số video: 2000 clips.
	Phân bố nhãn: 1000 video bạo lực (Violence) và 1000 video không bạo lực (NonViolence).
	Đặc điểm: Video có độ dài từ 2-10 giây, độ phân giải đa dạng từ 240p đến 720p.
 
Hình 3.7: Phân tích thống kê thời điểm xảy ra hành vi bạo lực trong tập dữ liệu RLVS
	Đặc điểm nội dung:
	Video bạo lực (Violence): Bao gồm các tình huống đánh nhau trên đường phố, xô xát nơi công cộng, bạo lực gia đình, và các hành vi tấn công thể chất trong nhiều bối cảnh khác nhau.
	Video không bạo lực (NonViolence): Bao gồm các hoạt động thường ngày như đi bộ, chạy bộ, chơi thể thao, múa/nhảy, tập thể dục, và các tương tác xã hội bình thường.
	Thách thức: Đây là bộ dữ liệu có độ đa dạng cao nhất trong ba bộ được sử dụng. Các video được quay từ nhiều góc độ khác nhau (góc nghiêng, góc cao, POV), với điều kiện ánh sáng biến đổi (ban ngày, ban đêm, trong nhà, ngoài trời), và chất lượng video không đồng đều do được thu thập từ nhiều nguồn khác nhau. Điều này tạo ra thách thức lớn cho mô hình trong việc tổng quát hóa trên dữ liệu thực tế.
	Điểm mạnh: Tính thực tiễn cao - video phản ánh chính xác các tình huống bạo lực xảy ra trong đời thực, giúp mô hình học được các đặc trưng bạo lực trong môi trường không kiểm soát.
Bộ dữ liệu hợp nhất (Unified Violence Dataset - UVD)
	Để khắc phục hạn chế về quy mô của các bộ dữ liệu đơn lẻ và tăng cường khả năng tổng quát hóa của mô hình, luận văn đề xuất xây dựng một bộ dữ liệu hợp nhất (Unified Violence Dataset - UVD). UVD là sự kết hợp của ba nguồn dữ liệu uy tín: RWF-2000, Hockey Fight và Real Life Violence Situations (RLVS).
	Thành phần:
	RWF-2000: 2000 video (Camera giám sát).
	Hockey Fight: 1000 video (Thể thao).
	RLVS [6]: 2000 video (Thực tế đa dạng).
	Tổng cộng: 5000 video clips, được cân bằng hoàn hảo với 2500 video bạo lực và 2500 video không bạo lực. Đây là một trong những bộ dữ liệu lớn nhất hiện nay cho bài toán này, giúp mô hình học được các đặc trưng đa dạng từ nhiều môi trường khác nhau.
 
Hình 3.8: Một số mẫu khung hình từ tập dữ liệu UVD
 
Hình 3.9: Biểu đồ tròn thành phần bộ dữ liệu UVD
 
Hình 3.10: Biểu đồ phân bố nhãn UVD
Quy trình xử lý dữ liệu
	Quy trình xử lý dữ liệu được thiết kế để chuẩn hóa video đầu vào, nhấn mạnh các hành vi chuyển động, và làm dữ liệu đa dạng để mô hình học tốt hơn. Quy trình gồm ba bước chính: trích xuất và chuẩn hóa khung hình, trích xuất chuyển động, tăng cường dữ liệu
Trích xuất và chuẩn hóa khung hình
	Các đoạn video từ ba bộ dữ liệu thành phần (RWF-2000, Hockey-Fight, RLVS) có độ dài và tốc độ khung hình (FPS) rất khác nhau. Ví dụ, Hockey Fight có FPS cao và diễn biến nhanh, trong khi RWF-2000 có FPS thấp hơn. Do đó, việc lấy mẫu cố định theo giây (ví dụ: lấy 10 frame/giây) sẽ dẫn đến sự mất cân đối về lượng thông tin đầu vào giữa các video.
	Để giải quyết vấn đề này, luận án áp dụng kỹ thuật Lấy mẫu đồng nhất theo thời gian (Uniform Temporal Sampling). Cụ thể, mỗi video đầu vào V sẽ được chia thành T=16 đoạn (segments) có độ dài bằng nhau. Từ mỗi đoạn, hệ thống sẽ chọn ngẫu nhiên một khung hình để đại diện. Kết quả là một chuỗi 16 khung hình (T=16) bao quát toàn bộ diễn biến thời gian của video, bất kể video đó dài 2 giây hay 10 giây. Kỹ thuật này giúp chuẩn hóa đầu vào cho mô hình và đảm bảo tính nhất quán của dữ liệu.
	Tiếp theo, các khung hình này được thay đổi kích thước về 224×224 pixels để phù hợp với backbone MobileNet đã lựa chọn, đồng thời được chuyển về không gian màu RGB. Cuối cùng, tất cả các pixel của khung hình được chuẩn hóa về khoảng [0,1] bằng cách chia giá trị pixel cho 255. Việc chuẩn hóa này giúp mô hình hội tụ nhanh hơn và cải thiện hiệu suất huấn luyện, đồng thời giữ nguyên tỉ lệ và nội dung hình ảnh để bảo toàn thông tin ban đầu.
Trích xuất chuyển động
	Các khung hình chuẩn hóa từ bước trước được sử dụng để tính chuyển động giữa các khung hình liên tiếp bằng phương pháp Optical Flow (tính toán tốc độ và hướng chuyển động giữa hai khung hình liên tiếp), giúp mô hình nhận biết các thay đổi động trong video. Các vùng chuyển động nổi bật được xác định và trích xuất, chỉ giữ lại các phần chuyển động quan trọng trong khung hình. Kết quả là 30 khung hình chuyển động, nhấn mạnh các hành vi quan trọng, sẵn sàng để đưa vào module trích xuất đặc trưng tiếp theo.
Tăng cường dữ liệu 
	Để cải thiện khả năng tổng quát của mô hình, các khung hình trong tập huấn luyện được áp dụng các kỹ thuật tăng cường như: cắt ngẫu nhiên (Random Crop), lật ngang (Horizontal Flip), thay đổi độ sáng, độ tương phản, độ bão hòa (Color Jitter), xoay ±10° (Rotation), và làm mờ nhẹ (Gaussian Blur). Những bước này giúp mô hình học được nhiều biến thể của hành vi bạo lực, tăng khả năng chống chịu với các thay đổi về vị trí, hướng hành động, ánh sáng, hoặc chất lượng video, từ đó giảm nguy cơ overfitting và nâng cao hiệu quả tổng quát trên các video mới.
CÁC THUẬT TOÁN ĐƯỢC SỬ DỤNG
	Quy trình xử lý dữ liệu video đầu vào được thực hiện tuần tự qua ba giai đoạn chính: trích xuất chuyển động, trích xuất đặc trưng và phân loại hành vi.
Chi tiết thuật toán lõi
Giai đoạn 1: Trích xuất Chuyển động Không gian
	Để loại bỏ nhiễu nền và tập trung vào các đối tượng chuyển động, hệ thống trước tiên sử dụng phương pháp tính toán sự thay đổi cường độ điểm ảnh giữa các khung hình liên tiếp dựa trên khoảng cách Euclid, sau đó là áp dụng phương pháp tạo mặt nạ và lọc nhiễu (được thể hiện qua hình 3.2).
	Tính toán khoảng cách Euclid:
	Với mỗi cặp khung hình liên tiếp F_t và F_{t+1}, hệ thống tính toán sự khác biệt tại từng điểm ảnh dựa trên khoảng cách Euclid giữa các kênh màu RGB. Công thức được định nghĩa như sau:
d_t=\sqrt{\sum_{i=1}^{3}\left(F_{t+1}^i-F_t^i\right)^2}
(Trong đó i chạy từ 1 đến 3 đại diện cho các kênh RGB; F_t^i là giá trị màu của điểm ảnh tại thời điểm t)
	Tạo mặt nạ và lọc nhiễu:
	Các điểm ảnh có sự thay đổi lớn (giá trị d_t vượt ngưỡng) được xác định là vùng chuyển động. Hệ thống áp dụng phép giãn nở hình thái học để làm liền mạch các vùng này, sau đó nhân với khung hình gốc để tạo ra các khung hình chuyển động.
 
Hình 3.11: Mã giả cho Giai đoạn 1
Giai đoạn 2: Trích xuất đặc trưng không gian - thời gian
	Giai đoạn này chuyển đổi dữ liệu thô từ các khung hình chuyển động thành các đặc trưng cấp cao, phục vụ cho việc nhận dạng.
	Tổng hợp theo thời gian:
	Để giảm tải tính toán, dữ liệu được nén từ 30 xuống còn 10 khung hình. Cứ mỗi nhóm 3 khung hình liên tiếp sẽ được gộp lại bằng cách tính giá trị trung bình. Cách này giúp giữ lại thông tin diễn biến theo thời gian nhưng giảm kích thước dữ liệu đầu vào.
	Trích xuất đặc trưng không gian:
	10 khung hình tổng hợp sau đó được chuẩn hóa và đưa qua mạng nơ-ron tích chập MobileNetV3 Small. Tại lớp tích chập cuối cùng, mỗi khung hình tạo ra một bản đồ đặc trưng kích thước (576, 7, 7) (như hình dưới).
 
Hình 3.12: Mã giả cho Giai đoạn 2
Giai đoạn 3: Phân loại hành vi
	Khối dữ liệu đặc trưng từ Giai đoạn 2 được xử lý qua cơ chế trọng số và lớp phân loại (hình 3.4) để đưa ra kết luận:
	Nén không gian: Áp dụng kỹ thuật gộp trung bình toàn cục chiều không gian (7 x 7), biến đổi khối dữ liệu thành ma trận kích thước (576, 10).
	Tính trọng số thời gian: Để xác định khung hình nào chứa thông tin quan trọng (ví dụ: khoảnh khắc cú đấm tiếp xúc), hệ thống sử dụng một mạng nơ-ron nhỏ gồm hai lớp kết nối đầy đủ (Fully Connected - FC) để học trọng số \alpha_t:
	Bước nén: Vector đặc trưng đi qua lớp FC thứ nhất và hàm kích hoạt ReLU để nén thông tin và giới thiệu tính phi tuyến.
	Bước kích hoạt: Kết quả đi qua lớp FC thứ hai và hàm Sigmoid để đưa giá trị về khoảng (0, 1).
	Công thức:
\alpha_t=\sigma\left(W_2\cdot\delta\left(W_1\cdot v_t\right)\right)
(Trong đó: \sigma là hàm Sigmoid, \delta là hàm ReLU, W_1 và W_2 là trọng số các lớp FC)
	Tổng hợp đặc trưng: Vector đặc trưng của mỗi khung hình v_t được nhân với trọng số \alpha_t tương ứng (Scale). Sau đó, hệ thống cộng gộp tất cả các vector lại để tạo ra vector đại diện duy nhất V_{final} cho toàn bộ đoạn video:
V_{final}=\sum_{t=1}^{10}\left(\alpha_t\cdot v_t\right)
	Phân loại: Vector V_{final} được đưa qua lớp phân loại cuối cùng và hàm Softmax để tính xác suất bạo lực.
 
Hình 3.13: Mã giả cho Giai đoạn 3
Các thuật toán cho Suy luận
Thuật toán Cửa sổ trượt và Quản lý bộ đệm
Để xử lý luồng video liên tục từ Kafka, module AI Service duy trì một bộ đệm cục bộ (local buffer) chứa 30 khung hình gần nhất cho mỗi camera. Cơ chế cửa sổ trượt (Sliding Window) đảm bảo mô hình luôn có đủ ngữ cảnh thời gian để đưa ra dự đoán. [ĐÃ CẬP NHẬT]
	Giai đoạn khởi động (Warm-up Phase): Khi hệ thống mới bắt đầu, bộ đệm chưa đủ 30 khung hình. Trong giai đoạn này, hệ thống chỉ thu thập dữ liệu mà không thực hiện dự đoán để tránh lỗi kích thước đầu vào.
	Chiến lược bước trượt (Stride Strategy):
	Stride = 1: Dự đoán dày đặc trên từng khung hình.
	Stride > 1: "Nhảy cóc" khung hình để giảm tải tính toán.
 
Hình 3.14: Mã giả cho Thuật toán Cửa sổ trượt và Quản lý bộ đệm
Thuật toán Hậu xử lý và Làm trơn kết quả
Dự đoán đơn lẻ thường không ổn định do nhiễu. Thuật toán làm trơn dựa trên trung bình di động được áp dụng để tăng độ tin cậy.
	Phân tích tham số K (Window Size): Kích thước hàng đợi lịch sử (K) ảnh hưởng trực tiếp đến hiệu năng:
	K nhỏ: Hệ thống phản hồi nhanh (nhạy) nhưng dễ bị nhiễu (báo động giả).
	K lớn: Kết quả ổn định, mượt mà nhưng gây ra độ trễ trong việc phát hiện sự kiện.
	Trong hệ thống này, K=5\ được chọn để cân bằng giữa độ ổn định và tốc độ phản hồi.
 
Hình 3.15: Thuật toán Hậu xử lý và Làm trơn kết quả
Lập lịch xử lý bất đồng bộ (Asynchronous Scheduling)
Hệ thống tách biệt luồng thu thập (Camera Worker) và luồng xử lý (AI Worker) thông qua hàng đợi Kafka để tối ưu hóa FPS và ngăn chặn hiện tượng Back-pressure (nghẽn ngược). [ĐÃ CẬP NHẬT]
	Chiến lược Frame Dropping: Khi hàng đợi đầy, hệ thống loại bỏ khung hình mới nhất để tránh tích tụ độ trễ.
 
Hình 3.16: Mã giả cho Lập lịch xử lý bất đồng bộ
Các thuật toán cho Huấn luyện
Giải thuật tăng cường dữ liệu
	Trước khi đưa vào mạng nơ-ron, dữ liệu video được xử lý qua pipeline tăng cường nhằm giảm thiểu hiện tượng quá khớp và tăng tính tổng quát hóa.
	Quy trình áp dụng augmentation: Augmentation được áp dụng SAU khi module SME tính toán luồng quang (Optical Flow), để đảm bảo các vector chuyển động được tính chính xác từ khung hình gốc. Tiếp đó, các phép biến đổi được áp dụng lần lượt với xác suất cụ thể:
	Temporal Jitter (15%): Dịch chuyển ngẫu nhiên chỉ số khung hình trong khoảng \pm2\ frames. Kỹ thuật này thêm biến thiên thời gian, giúp mô hình không bị quá khớp vào các mẫu chuỗi cố định.
	Random Crop (35%): Cắt ngẫu nhiên một vùng từ kích thước gốc 224\ \times224 xuống 212\ \times212, sau đó thay đổi kích thước (resize) trở lại 224\ \times224. Điều này giúp mô hình học được các đặc trưng ở nhiều vị trí không gian khác nhau.
	Horizontal Flip (30%): Lật ngang khung hình để mô hình không phụ thuộc vào hướng của hành động (trái/phải).
	Color Jitter (25%): Thay đổi ngẫu nhiên độ sáng, độ tương phản và độ bão hòa để mô hình bất biến với điều kiện ánh sáng.
 
Hình 3.17: Mã giả cho Giải thuật tăng cường dữ liệu
Vòng lặp huấn luyện
	Quá trình huấn luyện sử dụng thuật toán Adam kết hợp với chiến lược OneCycleLR scheduler. Đầu vào của vòng lặp là các đặc trưng không gian - thời gian (STE features) đã được trích xuất và lưu trữ (cache) trước đó để tối ưu tốc độ.
 
Hình 3.18: Mã giả cho Vòng lặp huấn luyện
	Quy trình mỗi Epoch:
	Bước 1 - Forward Pass: Đưa batch gồm 8 video features qua mô hình GTE (Global Temporal Extractor) để tính toán logits dự đoán.
	Bước 2 - Loss Computation: Tính hàm mất mát CrossEntropyLoss bằng cách so sánh kết quả dự đoán với nhãn thực tế (ground truth).
	Bước 3 - Backward Pass: Tính gradient của loss đối với tất cả các tham số của GTE bằng thuật toán lan truyền ngược (backpropagation).
	Bước 4 - Update Parameters: Trình tối ưu hóa Adam cập nhật trọng số dựa trên gradient đã tính.
	Bước 5 - Learning Rate Scheduling: OneCycleLR tự động cập nhật tốc độ học sau mỗi bước (step).
	Bước 6 - Validation: Sau mỗi epoch, đánh giá mô hình trên tập validation. Nếu độ chính xác (accuracy) tốt hơn kết quả tốt nhất trước đó, tiến hành lưu checkpoint. Nếu không cải thiện trong 10 epochs liên tiếp, kích hoạt dừng sớm (Early Stopping).
Thuật toán tối ưu hóa (Adam Optimizer)
	Hệ thống sử dụng thuật toán Adam (Adaptive Moment Estimation) để cập nhật trọng số. Đây là phương pháp tối ưu dựa trên gradient, kết hợp ưu điểm của AdaGrad và RMSProp. Để kiểm soát hiện tượng quá khớp (overfitting), kỹ thuật suy giảm trọng số (Weight Decay) được áp dụng trực tiếp vào bước cập nhật tham số (tương đương với AdamW).
	Các siêu tham số thiết lập:
	Tốc độ học (Learning rate) \alpha:1\times{10}^{-3}
	Hệ số suy giảm moment \left(\beta_1,\beta_2\right):\left(0.9,0.999\right)
	Hằng số ổn định \epsilon:1\times{10}^{-9}
	Hệ số Weight Decay \lambda:\ 0.01
	Công thức toán học chi tiết: Tại bước huấn luyện thứ t, giả sử g_t là gradient của hàm mất mát đối với tham số \theta\left(g_t=\nabla_\theta J\left(\theta_{t-1}\right)\right). Quy trình cập nhật diễn ra như sau:
	Cập nhật các moment (Momentum Update):
m_t=\beta_1m_{t-1}+\left(1-\beta_1\right)g_t
v_t=\beta_2v_{t-1}+\left(1-\beta_2\right)g_t^2
Trong đó:
	m_t: Ước lượng moment bậc 1 (trung bình trượt của gradient) tại bước t.
	v_t: Ước lượng moment bậc 2 (trung bình trượt của bình phương gradient) tại bước t.
	g_t^2: Bình phương từng phần tử của vector gradient (element-wise square).
	Hiệu chỉnh độ chệch (Bias Correction):
	Do m_0 và v_0 được khởi tạo bằng 0, chúng có xu hướng bị lệch về 0 trong các bước đầu. Bước này giúp khắc phục điều đó:
\widehat{m_t}=\frac{m_t}{1-\beta_1^t}
\widehat{v_t}=\frac{v_t}{1-\beta_2^t}
Trong đó:
	\widehat{m_t},\widehat{v_t}: Các giá trị moment đã được hiệu chỉnh bias.
	\beta^t: Lũy thừa bậc t của hệ số \beta.
	Cập nhật tham số (Parameter Update):
	Trọng số mới được tính toán bằng cách kết hợp bước nhảy Adam và thành phần Weight Decay:
\theta_t=\theta_{t-1}-{\underbrace{\alpha\frac{\widehat{m_t}}{\sqrt{\widehat{v_t}}+\epsilon}}}\below{\mathrm{Adam\ Step}}-{\underbrace{\alpha\lambda\theta_{t-1}}}\below{\mathrm{Weight\ Decay}}
Trong đó:
	\theta_t: Giá trị tham số mô hình tại bước $t$.
	\alpha: Tốc độ học (Learning rate).
	\epsilon: Hằng số nhỏ để tránh lỗi chia cho 0.
	\lambda: Hệ số suy giảm trọng số (L2 regularization factor).
 
Hình 3.19: Mã giả cho Thuật toán tối ưu hoá
Điều chỉnh tốc độ học (OneCycleLR Scheduler)
	Chiến lược OneCycleLR điều chỉnh tốc độ học \eta linh hoạt theo từng batch (bước lặp), thay vì giữ cố định hoặc chỉ giảm theo epoch. Đồ thị tốc độ học có dạng "một chu kỳ" gồm hai giai đoạn:
	Giai đoạn Tăng tốc (Warm-up): Tăng tuyến tính từ giá trị rất nhỏ lên cực đại.
	Giai đoạn Giảm tốc (Annealing): Giảm theo hàm Cosine từ cực đại về giá trị nhỏ hơn khởi điểm.
 
Hình 3.20: Mã giả cho Điều chỉnh tốc độ học
3.4	MODULE PHÂN TÍCH VÀ DỰ BÁO (ANALYTICS ENGINE)
	Đây là điểm nhấn công nghệ quan trọng, giúp nâng tầm hệ thống từ một công cụ "ghi hình thụ động" trở thành một trợ lý an ninh "chủ động phòng ngừa". Module này hoạt động như một "Bộ não thứ hai" - Module Phân tích Nâng cao (Advanced Analytics Engine), chuyên trách xử lý dữ liệu có cấu trúc (Structured Data) được trích xuất từ luồng video.
3.4.1	Quy trình xử lý dữ liệu Analytics
	Khác với Module Nhận diện Hành vi (Video Processing Pipeline), quy trình Analytics tập trung vào việc khai phá tri thức từ Metadata sự kiện.
	1. Tiền xử lý dữ liệu (Data Preprocessing):
	Dữ liệu thô từ log bao gồm `timestamp`, `camera_id`, và `confidence`. Để đưa vào các thuật toán học máy, chúng cần được chuyển đổi:
	- Gán nhãn (Label Encoding): Chuyển đổi dữ liệu định danh (ví dụ: Camera_01, Monday) thành các vector số học.
	- Chuẩn hóa (Standardization): Sử dụng phương pháp StandardScaler để đưa các biến số về cùng phân phối chuẩn (mean=0, std=1), giúp các thuật toán dựa trên khoảng cách (như K-Means) hoạt động hiệu quả và hội tụ nhanh hơn.
	2. Luồng thực thi (Execution Flow):
	Dữ liệu sau khi làm sạch được định tuyến đến hai pipeline song song:
	- Pipeline Phân cụm (Clustering): Gom nhóm sự kiện để tìm ra quy luật không gian - thời gian.
	- Pipeline Dự báo (Risk Prediction): Đánh giá mức độ rủi ro dựa trên dữ liệu quá khứ.
3.4.2	Thuật toán Phân cụm (K-Means) triển khai
	Sử dụng thuật toán K-Means (đã trình bày ở 2.7.3) để tự động phân cụm hàng nghìn sự kiện bạo lực. 
	- Input: Vector đặc trưng `[Giờ, Ngày, Vị trí, Độ tin cậy]`.
	- Process: Thuật toán tìm các tâm cụm (Centroids) đại diện cho các "điểm nóng".
	- Output: Danh sách các Cluster Cards hiển thị trên Dashboard, giúp người quản lý nhận diện nhanh các khu vực/khung giờ trọng điểm mà không cần xem lại từng video.
3.4.3	Thuật toán Dự báo (Random Forest) triển khai
	Sử dụng mô hình Random Forest Classifier (đã trình bày ở 2.7.2) để xây dựng chức năng cảnh báo sớm.
	- Input: Thời gian (Giờ tiếp theo) và Địa điểm (Camera ID).
	- Process: Mô hình tổng hợp kết quả bình chọn (Voting) từ hàng trăm cây quyết định để đưa ra xác suất rủi ro.
	- Output: Nhãn cảnh báo Rủi ro (Cao/Trung bình/Thấp) giúp hệ thống chủ động điều phối nguồn lực an ninh trước khi sự việc xảy ra.

3.5	THIẾT KẾ LOGIC ỨNG DỤNG
	Hệ thống ứng dụng được xây dựng để hỗ trợ quy trình nghiệp vụ giám sát an ninh... (như cũ).
3.5.1	Logic xử lý tại Backend (Application Logic)
	Thay vì sử dụng kiến trúc API Server truyền thống (nơi server nhận request và gọi trực tiếp đến AI model), luận án đề xuất và triển khai kiến trúc "Streaming Data Pipeline" với Apache Kafka đóng vai trò trung tâm (Backbone).
	Kiến trúc Kafka-Centric (Kafka làm trung tâm):
	Quy trình xử lý dữ liệu được tổ chức lại hoàn toàn theo luồng Streaming:
	1. Nhà sản xuất dữ liệu (Data Producer - Camera Worker):
	Module này kết nối trực tiếp với RTSP Server của Camera thông qua thư viện OpenCV. Nhiệm vụ duy nhất của nó là đọc (Read) các khung hình (Frames) liên tục, đóng gói chúng thành các bản tin nhị phân (sử dụng MessagePack để tối ưu kích thước) và đẩy (Produce) vào một Topic Kafka chuyên biệt (ví dụ: `camera_frames`).
	2. Đường ống dữ liệu (Data Broker - Kafka):
	Kafka đóng vai trò là "mạch máu" lưu thông dữ liệu của toàn bộ hệ thống. Nó nằm giữa lớp thu thập và lớp xử lý, hoạt động như một bộ đệm khổng lồ. Điều này giúp hệ thống tách biệt hoàn toàn (Decoupling) giữa tốc độ thu thập video (ổn định 30 FPS) và tốc độ xử lý của AI (có thể biến động).
	3. Người tiêu thụ dữ liệu (AI Consumer):
	AI Service được thiết kế như một Worker độc lập. Nó không kết nối với Camera mà đăng ký (Subscribe) vào Topic tương ứng trên Kafka để lấy dữ liệu về xử lý.
	Lợi ích của kiến trúc này:
	- Khả năng mở rộng (Scalability): Có thể dễ dàng thêm nhiều AI Worker để xử lý song song hàng trăm camera mà không cần sửa đổi code của phần thu thập.
	- Độ tin cậy (Reliability): Nếu AI Service gặp sự cố hoặc khởi động lại, dữ liệu frame vẫn nằm an toàn trong Kafka, không bị mất mát, cho phép hệ thống "tự phục hồi" và xử lý đuổi (catch-up) sau đó.
3.5.2	Kiến trúc Microservices và Xử lý dữ liệu lớn
	Bên cạnh Kafka, hệ thống tích hợp các công nghệ Big Data để tối ưu hóa lưu trữ và truy xuất:
	- Redis (Pub/Sub & Cache): Đóng vai trò lớp đệm tốc độ cao cho các cảnh báo thời gian thực. Khi AI phát hiện bạo lực, kết quả được đẩy ngay lập tức vào Redis Pub/Sub để Frontend (Mobile/Web) nhận được với độ trễ gần như bằng 0 (< 5ms). Đồng thời, Redis lưu trữ trạng thái ngắn hạn của các camera.
	- HDFS (Hadoop Distributed File System): Đóng vai trò kho lưu trữ dài hạn (Data Lake). Mọi kết quả phân tích từ AI (bao gồm cả Bạo lực và Không bạo lực, cùng với log hệ thống) được Kafka Consumer tự động đẩy xuống HDFS. Kho dữ liệu này là nguyên liệu quý giá cho việc Huấn luyện lại (Retraining) mô hình sau này, tạo thành vòng lặp MLOps khép kín giúp mô hình ngày càng thông minh hơn.
3.5.3	Logic hiển thị và tương tác (Frontend Logic)

	Ứng dụng di động (Flutter) và Web Dashboard được thiết kế để cung cấp trải nghiệm giám sát liền mạch.
	Kết nối thời gian thực (WebSocket): Ứng dụng duy trì một kết nối hai chiều bền vững với server. Điều này cho phép server chủ động "đẩy" cảnh báo xuống ứng dụng ngay khi sự kiện xảy ra, thay vì ứng dụng phải liên tục "hỏi" server (polling), giúp tiết kiệm pin và băng thông.
	Hiển thị bằng chứng (Evidence Playback): Khi nhận được cảnh báo, ứng dụng không chỉ rung/chuông mà còn tự động tải và phát đoạn video clip ngắn ghi lại khoảnh khắc bạo lực vừa xảy ra. Điều này giúp nhân viên an ninh xác minh tình huống nhanh chóng trước khi đưa ra hành động can thiệp.
 
THỰC NGHIỆM VÀ KẾT QUẢ
MÔI TRƯỜNG THỰC NGHIỆM
	Để đảm bảo tính khách quan và khả năng tái lập của các kết quả, các thí nghiệm được thực hiện trên một môi trường phần cứng và phần mềm thống nhất.
Cấu hình phần cứng
	CPU: Intel Core i5-13400F (10 cores, 16 threads).
	GPU: NVIDIA GeForce RTX 3060 TI (8GB VRAM) - Sử dụng cho huấn luyện và suy luận.
	RAM: 32GB DDR4 3200MHz.
	Lưu trữ: SSD NVMe 1TB.
Cấu hình phần mềm
	Hệ điều hành: Windows 11.
	Framework: PyTorch 2.1.0, Torchvision 0.16.0.
	Thư viện hỗ trợ: OpenCV 4.8, NumPy 1.24, Scikit-learn 1.3.
	CUDA: Version 12.1 (hỗ trợ tăng tốc phần cứng).
Hạ tầng triển khai
	Hệ thống được đóng gói và triển khai hoàn toàn trên môi trường Docker để đảm bảo tính nhất quán giữa môi trường phát triển và vận hành:
	Docker Compose: Quản lý và điều phối 10+ containers bao gồm Backend, AI Service, Kafka, Zookeeper, Redis, Frontend, và các Camera giả lập.
	Networking: Sử dụng Docker Bridge Network riêng biệt để tối ưu hóa giao tiếp nội bộ giữa các microservices.
	Volume Mapping: Dữ liệu quan trọng (model weights, logs, database) được mount ra ngoài container để đảm bảo an toàn dữ liệu.
Tham số huấn luyện
	Tốc độ học: 10-3 cho mọi bộ dữ liệu.
	Kích thước lô dữ liệu: 16.
	Số vòng huấn luyện: 100.
	Thuật toán tối ưu hóa: Adam, với hệ số Epsilon là 10-9; độ suy giảm trọng số là 10-2; sử dụng hàm mất mát Cross Entropy.
	Cơ chế điều chỉnh tốc độ học: One-Cycle Learning Rate với giai đoạn khởi động chiếm 30% tổng số thời gian và hệ số chia khởi điểm là 25.
KẾT QUẢ THỰC NGHIỆM CHI TIẾT
So sánh hiệu năng các Backbone
	Thí nghiệm đầu tiên nhằm mục đích lựa chọn mạng xương sống tối ưu cho module STE. Luận án đã huấn luyện và so sánh ba kiến trúc phổ biến: MobileNetV2, MobileNetV3-Small, và EfficientNet-B0 trên cùng tập dữ liệu RWF-2000.
Bảng 4.1: So sánh hiệu năng và tài nguyên giữa các Backbone
Backbone	Tham số (M)	FLOPs (G)	Độ chính xác (%)	Thời gian xử lý (ms)
MobileNetV2	3.51	3.51	81	12
MobileNetV3-Small	2.54	1.25	82	8
MobileNetV3 Large	7.62	4.1	84.8	15
EfficientNet-B0	5.29	4.17	87.8	18
MNasNet	2.22	1.13	83.3	17
	Nhận xét: MobileNetV3-Small cho kết quả độ chính xác cao nhất (82%) đồng thời có thời gian suy luận thấp nhất (8ms), chứng tỏ sự hiệu quả của kiến trúc tìm kiếm mạng thần kinh (NAS). Đây là lý do luận án chọn MobileNetV3-Small làm backbone chính thức để tối ưu hóa cho các thiết bị biên.
Kết quả trên tập dữ liệu RWF-2000
	RWF-2000 là tập dữ liệu thách thức nhất với nhiều tình huống thực tế phức tạp.
 
Hình 4.1: Biểu đồ Accuracy và Loss trong quá trình huấn luyện trên RWF-2000
 
Hình 4.2: Ma trận nhầm lẫn (Confusion Matrix) trên tập Test RWF-2000
Kết quả trên tập dữ liệu Hockey Fight
	Trên tập dữ liệu Hockey Fight, mô hình đạt kết quả rất cao do môi trường và góc quay ổn định hơn.

 
Hình 4.3: Biểu đồ Accuracy và Loss trong quá trình huấn luyện trên Hockey Fight
 
Hình 4.4: Ma trận nhầm lẫn trên tập Hockey Fight
	Kết quả này khẳng định tính đúng đắn của thuật toán SME trong việc trích xuất chuyển động, đặc biệt hiệu quả trong các video thể thao có nền tĩnh và chuyển động rõ ràng.
Kết quả trên bộ dữ liệu hợp nhất (UVD)
	Đây là thực nghiệm quan trọng nhất nhằm đánh giá khả năng tổng quát hóa của mô hình trên quy mô dữ liệu lớn (5000 video).
	Quá trình huấn luyện diễn ra trong 100 Epochs.
 
Hình 4.5: Biểu đồ Accuracy và Loss trên tập UVD
 
Hình 4.6: Ma trận nhầm lẫn trên tập Test
   Các chỉ số đánh giá mô hình trên tập UVD:
	Recall (Độ nhạy – Bắt trúng bạo lực): 87,78%
	Precision (Độ chính xác – Ít báo động giả): 87,78%
   Tổng quan: mô hình có xu hướng bắt bạo lực khá nhạy nhưng đôi khi hơi “nhạy cảm quá” nên thường xảy ra việc nhầm video thường thành bạo lực. Nhưng nét xét về bài toán nhận dạng bạo lực và trong những trường hợp giám sát an ninh thực tế, việc ít bỏ sót tội phạm thường quan trọng hơn, nên có thể nhận xét rằng đây là kết quả đi theo chiều hướng tích cực.
Kết quả ứng dụng thực tế
	Để đánh giá khả năng chịu tải và độ trễ của hệ thống trong điều kiện vận hành thực tế, luận án đã thiết lập một kịch bản kiểm thử với 4 luồng Camera IP (RTSP) hoạt động đồng thời.
	Cấu hình thử nghiệm:
	Server: Cấu hình như mục 4.1 (GPU RTX 3060 Ti).
	Đầu vào: 4 Camera IP được giả lập bời FFmpeg kết nối qua giao thức RTSP.
	Cơ chế: Hệ thống xử lý song song 4 luồng video (4 Workers), sử dụng chung một Model Instance trên GPU.
	Phân tích độ trễ: Biểu đồ độ trễ ghi nhận được chia làm hai giai đoạn rõ rệt:
	Giai đoạn Khởi động (3 giây đầu):
	Khi hệ thống mới khởi động, độ trễ xử lý khá cao, dao động từ 150ms - 250ms.
	Nguyên nhân: Do quá trình khởi tạo CUDA Context, nạp trọng số mô hình vào VRAM, và các cơ chế tối ưu hóa JIT (Just-In-Time) của PyTorch cần thời gian để "làm nóng".
	Tuy nhiên, giai đoạn này diễn ra rất ngắn và không ảnh hưởng đến vận hành lâu dài.
	Giai đoạn Ổn định:
	Sau khoảng 3 giây, độ trễ giảm mạnh và đi vào ổn định.
	Độ trễ xử lý trung bình: ~15.5 ms/lần suy luận.
	Độ trễ End-to-End: ~27 ms.
Bảng 4.2: Phân rã độ trễ toàn trình
Thành phần	Thời gian (ms)	Mô tả
RTSP Decoding	~ 4 ms	Giải mã luồng video H.264 từ Camera.
Tiền xử lý	~ 3 ms	Resize, Normalize, xếp chồng khung hình.
Mô hình xử lý	~ 15 ms	Thời gian xử lý trên GPU (MobileNetV3 + SME).
System & Network	~ 5 ms	Truyền tin qua Redis Pub/Sub và WebSocket.
TỔNG CỘNG	~ 27 ms	Từ lúc sự kiện xảy ra đến lúc ứng dụng nhận cảnh báo.
	Đánh giá khả năng chịu tải:
	Với 4 Camera chạy ở 30 FPS, tổng số yêu cầu suy luận (Inference Requests) gửi đến GPU là: 4 * 30 = 120 (lần/giây)
	Dựa trên kết quả đo đạc đơn luồng (Inference FPS ~ 60), GPU RTX 3060 Ti có khả năng xử lý khoảng 60 yêu cầu/giây.
	Kết luận: Với tải thực tế là 24 yêu cầu/s so với năng lực 60 yêu cầu/s, hệ thống hoạt động ở mức ~40% công suất GPU. Điều này chứng minh hệ thống hoạt động rất mượt mà, không xảy ra hiện tượng nghẽn cổ chai, và hoàn toàn có thể mở rộng lên 6-8 camera trên cùng một phần cứng mà vẫn đảm bảo độ trễ thời gian thực (< 100ms) về việc phát hiện bạo lực cho cả quản lý và người dùng cuối.
SO SÁNH VỚI CÁC NGHIÊN CỨU LIÊN QUAN
Đánh giá kết quả dựa trên bộ dữ liệu Hockey Fight
	Kết quả huấn luyện của mô hình trong luận án được so sánh với các nghiên cứu trước đây dựa trên độ chính xác trên bộ dữ liệu Hockey Fight. Kết quả cụ thể được thể hiện trong bảng sau:
Bảng 4.3: So sánh độ chính xác giữa các mô hình trên bộ dữ liệu Hockey Fight
Mô hình	Độ chính xác (%)
ViF + OViF [7]	87.5 ± 1.7
Radon Transform [6]	90.1 ± 0.2
Mô hình	Độ chính xác (%)
Three streams + LSTM [12]		93.9
FightNet [11]	97.0
Hough Forests + CNN [6]	94.6 ± 0.6
ConvLSTM  [9]	97.1 ± 0.55
3D CNN end to end [8]	98.3 ± 0.81
3D-DenseNet [1]	97.0
Luận án đề xuất 	97.75
	Nhìn chung, kết quả của mô hình đề xuất đạt độ chính xác khá cao xếp thứ hai trong các phương pháp được khảo sát (đứng sau mô hình 3D CNN end to end). Tuy nhiên, phải kể đến mô hình 3D CNN end to end dựa trên mạng 3D CNN nên có thể chắc chắn rằng nó sẽ có tham số và FLOPs cao hơn mô hình mà luận án đề xuất; nên khi cân nhắc về vấn đề phát hiện bạo lực trên thời gian thực thì có thể không hoàn toàn thích hợp.
Đánh giá kết quả dựa trên bộ dữ liệu RWF – 2000
	Khác với bộ dữ liệu Hockey Fight (những video chắc chắn có người, độ dài ngắn, thường không có nền nhiễu), thì RWF – 2000 như đã trình bày ở 3.2, có độ dài trung bình (5s) và đa dạng hơn về khung cảnh, cũng như là góc camera nên sẽ được chọn làm bộ dữ liệu để so sánh hai vấn đề chính là độ chính xác và tốc độ suy luận. Kết quả cụ thể được thể hiện trong bảng sau:
Bảng 4.4: So sánh các thông số giữa các mô hình trên bộ dữ liệu RWF – 2000
Mô hình	Độ chính xác (%)	Tham số (M)	FLOPs (G)
I3D + RGB (Carreira et al.) [4]	85.57	12.3	55.7
Mô hình	Độ chính xác (%)	Tham số (M)	FLOPs (G)
I3D + Two Stream (Carreira et al.) [4]	81.75	24.6	-
I3D + Optical Flow (Carreira et al.) [4]	75.5	12.3	-
ConvLSTM (Sudhakaran et al.) [9]	77.0	94.8	14.4
SA+TA (Huillcen et al.) [2]	87.75	5.29	4.17
Luận án đề xuất	82	2.54	1.25
	Nhìn chung, kết quả của mô hình đề xuất đạt độ chính xác ở mức khá (82%), tuy không phải là cao nhất trong các phương pháp được khảo sát (thấp hơn so với mô hình SA+TA và I3D + RGB).
	Tuy nhiên, điểm vượt trội của mô hình đề xuất nằm ở hiệu quả tính toán và tối ưu tài nguyên. Với số lượng tham số chỉ 2.54 M và lượng tính toán (FLOPs) là 1.25 G, mô hình này nhẹ hơn rất nhiều so với các đối thủ (thấp hơn khoảng một nửa so với mô hình SA+TA và thấp hơn nhiều lần so với I3D). Do đó, khi cân nhắc về bài toán triển khai thực tế trên các thiết bị giới hạn tài nguyên hoặc yêu cầu phản hồi nhanh, mô hình đề xuất cho thấy sự phù hợp cao hơn nhờ sự cân bằng tốt giữa độ chính xác và tốc độ.
Đánh giá kết quả dựa trên bộ dữ liệu hợp nhất (UVD)
Bảng 4.5: So sánh các thông số giữa các mô hình trên bộ dữ liệu UVD
Mô hình	Độ chính xác (%)
SA+TA (Huillcen et al.) [2]	90.5
Luận án đề xuất	87.78
	Nhận xét và Đánh giá sự đánh đổi:
	Về độ chính xác: Mô hình SA+TA đạt độ chính xác rất cao (90.5%), vượt trội hơn mô hình đề xuất (87.78%) khoảng 2.72%. Điều này là dễ hiểu vì SA+TA sử dụng kiến trúc phức tạp hơn với cơ chế Attention đa tầng.
	Về hiệu năng: Đây là điểm mạnh tuyệt đối của mô hình đề xuất.
	Số lượng tham số: Mô hình đề xuất chỉ có 2.54 triệu tham số, nhỏ hơn 2 lần so với SA+TA (5.29 triệu). Điều này giúp giảm dung lượng lưu trữ và bộ nhớ RAM tiêu thụ.
	Khối lượng tính toán (FLOPs): Quan trọng nhất, mô hình đề xuất chỉ tốn 1.25 GFLOPs, thấp hơn gần 4 lần so với SA+TA (4.17 GFLOPs).
	Kết luận: Nếu mục tiêu là chạy trên Server mạnh với GPU khủng để đạt độ chính xác tối đa, SA+TA là lựa chọn tốt. Tuy nhiên, với mục tiêu của luận án là (thường có tài nguyên hạn chế), mô hình đề xuất là sự lựa chọn tối ưu hơn hẳn. Luận án nhận hy sinh một lượng nhỏ độ chính xác để đổi lấy tốc độ xử lý nhanh gấp 4 lần, đảm bảo khả năng cảnh báo tức thời.
PHÂN TÍCH ĐỊNH TÍNH
	Để hiểu rõ hơn về hoạt động của mô hình, luận án đã phân tích một số trường hợp cụ thể:
	Trường hợp phát hiện đúng (True Positive): Các video đánh nhau đường phố, xô đẩy mạnh được phát hiện chính xác với độ tin cậy > 0.9. Thuật toán SME đã khoanh vùng chính xác các vùng chuyển động hỗn loạn.
	Trường hợp báo động giả (False Positive): Một số video múa hoặc tập võ thuật bị nhận nhầm là bạo lực. Nguyên nhân là do các động tác mạnh và nhanh tương tự như đánh nhau. Tuy nhiên, tỷ lệ này thấp (< 5%).
	Trường hợp hạn chế kỹ thuật: Khi đối tượng ở quá gần camera (cự ly gần), phép giãn nở hình thái học trong module SME gây ra tác dụng phụ. Các vùng chuyển động bị chồng lấn lên nhau, tạo thành các khối mặt nạ quá khổ. Điều này làm mất đi các đặc trưng hình dáng chi tiết (như tay, chân), khiến mô hình bị nhiễu loạn thông tin và đưa ra dự đoán thiếu ổn định.
	Trường hợp bỏ sót (False Negative): Các hành vi bạo lực tinh vi (ví dụ: móc túi, đẩy nhẹ) đôi khi bị bỏ qua do biên độ chuyển động quá nhỏ, dưới ngưỡng nhạy của Optical Flow.
TRIỂN KHAI ỨNG DỤNG THỰC TẾ
Giao diện Trung tâm hệ thống
	 
Hình 4.7: Giao diện Trung tâm hệ thống
	Giao diện Trung tâm hệ thống, được minh họa trong Hình 4.7, là khu vực người dùng tương tác nhiều nhất và đóng vai trò trung tâm điều khiển của toàn bộ hệ thống. Tại đây, các chức năng trọng tâm như xem camera, cảnh báo, quản lý camera được tổ chức dưới dạng các thẻ lớn, giúp người dùng dễ quan sát và thao tác nhanh.
Giao diện Lịch sử cảnh báo
Giao diện Lịch sử được minh họa trong Hình 4.8, cung cấp khả năng truy xuất và xem lại các sự kiện bạo lực đã được hệ thống phát hiện. Danh sách sự kiện được trình bày dưới dạng bảng trực quan với các thông tin chi tiết như: Tên camera, hình ảnh thu nhỏ, thời gian xảy ra sự kiện. 
 
Hình 4.8: Giao diện lịch sử cảnh báo
Giao diện Danh sách Camera trên Mobile
 
Hình 4.9: Giao diện Danh sách Camera
	Giao diện "Cameras" (Hình 4.9) là màn hình chính của ứng dụng, nơi người dùng có thể theo dõi trực tiếp các luồng video từ hệ thống giám sát. Giao diện được thiết kế dạng lưới với các thẻ camera bo tròn hiện đại, hiển thị hình ảnh thu nhỏ thời gian thực cùng trạng thái kết nối (chấm xanh "Connected"). Thanh tìm kiếm đặt ở trên cùng giúp người dùng nhanh chóng lọc và truy cập vào camera mong muốn trong các hệ thống lớn.
Giao diện Danh sách Sự kiện trên Mobile
 
Hình 4.10: Giao diện Danh sách Sự kiện
	Giao diện "Events" (Hình 4.10) đóng vai trò là nhật ký an ninh bỏ túi, liệt kê toàn bộ các sự kiện bạo lực được hệ thống phát hiện. Các sự kiện được sắp xếp theo thời gian, phân nhóm rõ ràng theo "Hôm nay", "Tuần này", "Tháng này". Mỗi thẻ sự kiện hiển thị tóm tắt ngắn gọn: vị trí camera (ví dụ: "Detected at Back Yard"), thời gian xảy ra, và trạng thái xem. Thiết kế tối màu với các điểm nhấn màu xanh lá tạo cảm giác an toàn và dễ chịu cho mắt khi sử dụng vào ban đêm.
Giao diện Chi tiết Sự kiện và Phát lại
	Khi chọn một sự kiện cụ thể, người dùng được chuyển đến màn hình "Event Details" (Hình 4.11). Tại đây, video clip ghi lại hành vi bạo lực sẽ được phát lại tự động, giúp người dùng xác minh sự việc. Các thông tin chi tiết như Tên Camera, Thời gian, Trạng thái được hiển thị rõ ràng. Đặc biệt, tính năng "Report false detection" (Báo cáo nhận diện sai) được tích hợp ngay dưới trình phát video, cho phép người dùng phản hồi lại hệ thống để cải thiện độ chính xác của mô hình trong tương lai (cơ chế Active Learning).
 
Hình 4.11: Giao diện Chi tiết sự kiện và tính năng báo cáo sai
Giao diện Quản lý Tài khoản và Đăng nhập
	Để đảm bảo tính bảo mật và cá nhân hóa, ứng dụng cung cấp các màn hình Đăng nhập (Hình 4.12), Đăng ký (Hình 4.13 và Quản lý hồ sơ (Hình 4.14).
	Giao diện Profile cho phép người dùng cập nhật thông tin cá nhân, đổi mật khẩu và truy cập các cài đặt ứng dụng khác.
 
Hình 4.12: Giao diện Đăng nhập
 
Hình 4.13: Giao diện Đăng ký
 
Hình 4.14: Giao diện Quản lý hồ sơ cá nhân
 
KẾT LUẬN VÀ HƯỚNG PHÁT TRIỂN
KẾT LUẬN
	Luận án đã hoàn thành đầy đủ các mục tiêu đề ra: xây dựng hệ thống phát hiện bạo lực theo thời gian thực dựa trên luồng video RTSP, phát triển mô hình học sâu có độ chính xác cao, và triển khai trọn bộ hệ thống từ phần xử lý phía máy chủ đến các ứng dụng giao diện người dùng trên web và điện thoại.
	Mô hình đề xuất kết hợp các khối trích xuất đặc trưng theo từng giai đoạn (SME – STE – GTE) cùng backbone MobileNetV3 Small đã cho kết quả tốt trên ba bộ dữ liệu chuẩn là RWF-2000, Hockey Fight và UVD.
	Về thuật toán, mô hình đạt 82% độ chính xác trên RWF-2000, 97.75% độ chính xác trên Hockey Fight và 84.2% độ chính xác trên UVD chứng minh khả năng tổng quát hóa tốt trên nhiều miền dữ liệu khác nhau. Cấu trúc mô hình theo hướng module giúp hệ thống dễ mở rộng, tối ưu và tái sử dụng trong các bài toán nhận diện khác.
	Về mặt hệ thống, luận án đã xây dựng hoàn chỉnh pipeline xử lý video với độ trễ thấp, xử lý được nhiều luồng camera RTSP đồng thời và đưa ra cảnh báo gần như ngay lập tức. Phần máy chủ được đóng gói dưới dạng container, hỗ trợ nhiều loại camera IP và có khả năng mở rộng khi triển khai thực tế.
	Tổng thể, luận án đã xây dựng được một giải pháp phát hiện bạo lực khả thi, có độ chính xác tốt, tốc độ xử lý đáp ứng yêu cầu thời gian thực, phục vụ hiệu quả cho các nhu cầu giám sát an ninh tại trường học, bệnh viện, khu dân cư hay các khu vực công cộng.
HẠN CHẾ
	Hạn chế của mô hình:
	Nhạy cảm với trường hợp bị che khuất, dẫn đến suy giảm độ chính xác.
	Phụ thuộc vào điều kiện ánh sáng; video ban đêm hoặc ánh sáng yếu làm suy giảm chất lượng chuyển động.
	Chuyển động quá nhanh gây mờ hình, làm giảm đặc trưng nhận dạng. Hạn chế về dữ liệu
	Bộ dữ liệu UVD còn nhỏ và chưa bao phủ đủ tình huống thực tế.
	Hạn chế của hệ thống:
	Cần GPU để đạt tốc độ xử lý ổn định; chưa tối ưu cho CPU hoặc thiết bị biên.
	Cửa sổ thời gian (sliding window) có hiện tượng trùng lặp, đôi khi gây lặp cảnh báo.
	Hệ thống chưa xử lý tốt các vấn đề mạng như mất gói tin hoặc độ trễ dao động.
	Hạn chế về phạm vi ứng dụng:
	Chỉ tập trung vào bạo lực 1–1, chưa xử lý tốt các vụ việc có nhiều người.
	Chưa kết hợp theo dõi liên tục nhiều đối tượng hoặc nhận dạng danh tính.
HƯỚNG PHÁT TRIỂN
Tối ưu mô hình
	Rút gọn mô hình (lượng tử hóa, giảm độ chính xác số) để tăng tốc và giảm kích thước.
	Dùng mô hình lớn huấn luyện mô hình nhỏ (chuyển tri thức) để phù hợp thiết bị biên.
	Cắt tỉa mô hình nhằm giảm số phép tính và giảm độ trễ.
Ứng dụng trên thiết bị biên
	Triển khai trên các thiết bị giá rẻ như Jetson Nano, Raspberry Pi hoặc các bộ tăng tốc phần cứng.
	Tối ưu hóa pipeline để có thể chạy ổn trên CPU khi không có GPU.
Mở rộng phạm vi bài toán
	Phát hiện các hành vi khác: ngã, xô xát nhóm, mang theo vật nguy hiểm, đột nhập.
	Kết hợp với module phát hiện người và theo dõi nhiều đối tượng để phân tích bạo lực nhóm.
	Tích hợp nhận diện khuôn mặt để theo dõi hoặc xác định danh tính người vi phạm.
Phát triển hệ thống
	Bổ sung bảng điều khiển nâng cao: bản đồ nhiệt, dòng thời gian, lịch sử sự kiện, báo cáo tự động.
	Tích hợp hỗ trợ chuẩn ONVIF để kết nối đa dạng loại camera.
Cải thiện khả năng học của mô hình
	Cơ chế tự học trong quá trình vận hành dựa trên phản hồi người dùng.
	Tận dụng video chưa gắn nhãn để mở rộng dữ liệu bằng bán giám sát.
	Ưu tiên gắn nhãn cho các mẫu khó nhằm tăng hiệu quả huấn luyện.
Triển khai thực tế
	Kiểm thử hệ thống tại trường học, bệnh viện, nhà máy, siêu thị.
	Điều chỉnh ngưỡng phát hiện theo từng môi trường cụ thể.  
TÀI LIỆU THAM KHẢO
[1] H. A. Huillcen Baca, J. C. Gutierrez Caceres, and F. de Luz Palomino Valdivia, "Efficiency in human actions recognition in video surveillance using 3D CNN and DenseNet," in Proceedings of the Future of Information and Communication Conference, San Francisco, CA, USA, 2022, pp. 342–355.
[2] H. A. Huillcen Baca, F. de Luz Palomino Valdivia, I. S. Solis, M. A. Cruz, and J. C. G. Caceres, "Human Violence Recognition in Video Surveillance in Real-Time," in Proceedings of the Future of Information and Communication Conference, San Francisco, CA, USA, 2023, pp. 783–795.
[3] H. A. Huillcen Baca, F. L. P. Valdivia, and J. C. Gutierrez Caceres, "Efficient Human Violence Recognition for Surveillance in Real Time," Sensors, vol. 24, no. 2, p. 668, 2024.
[4] J. Carreira and A. Zisserman, "Quo vadis, action recognition? A new model and the kinetics dataset," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 2017, pp. 6299–6308.
[5] M. Cheng, K. Cai, and M. Li, "RWF-2000: An Open Large Scale Video Database for Violence Detection," in 2020 25th International Conference on Pattern Recognition (ICPR), Milan, Italy, 2021, pp. 4183-4190. (Dataset RWF-2000)
[6] M. Mustafa, "Real Life Violence Situations Dataset," Kaggle, 2020. [Online].
[7] O. Deniz, I. Serrano, G. Bueno, and T. K. Kim, "Fast violence detection in video," in Proceedings of the 2014 International Conference on Computer Vision Theory and Applications (VISAPP), Lisbon, Portugal, 2014, vol. 2, pp. 478–485.
[8] Y. Gao, H. Liu, X. Sun, C. Wang, and Y. Liu, "Violence detection using oriented violent flows," Image and Vision Computing, vol. 48, pp. 37–41, 2016.
[9] J. Li, X. Jiang, T. Sun, and K. Xu, "Efficient violence detection using 3d convolutional neural networks," in Proceedings of the 2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), Taipei, Taiwan, 2019, pp. 1–8.
[10] E. B. Nievas, O. D. Suarez, G. B. García, and R. Sukthankar, "Violence detection in video using computer vision techniques," in Computer Analysis of Images and Patterns, Seville, Spain, 2011, pp. 332-339. (Dataset Hockey Fight)
[11] S. Sudhakaran and O. Lanz, "Learning to detect violent videos using convolutional long short-term memory," in Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), Lecce, Italy, 2017, pp. 1–6.
[12] P. Zhou, Q. Ding, H. Luo, and X. Hou, "Violent interaction detection in video based on deep learning," Journal of Physics: Conference Series, vol. 844, p. 012044, 2017.
[13] Dong, Z.; Qin, J.; Wang, Y. Multi-stream deep networks for person to person violence detection in videos. In Proceedings of the Pattern Recognition: 7th Chinese Conference, CCPR 2016, Chengdu, China, 5–7 November 2016; Proceedings, Part I 7. Springer: Berlin/Heidelberg, Germany, 2016; pp. 517–531.

