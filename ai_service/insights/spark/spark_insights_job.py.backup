"""
Spark Insights Job - Camera Credibility Training Pipeline

Implements the Camera Intelligence system:
1. K-Means: Clustering camera behaviors (Noisy/Reliable/Selective)  
2. FP-Growth: Mining false alarm patterns from verified data
3. Random Forest: Predicting camera credibility scores (0-1)

Requires verified data with human labels (true_positive/false_positive).
Artifacts are saved to: /app/ai_service/insights/data/
"""

import os
import json
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.types import ArrayType, StringType

logger = logging.getLogger(__name__)

# Constants
ARTIFACTS_DIR = "/app/ai_service/insights/data"

# Camera Mapping
CAMERA_NAMES = {
    "cam1": "Luy Ban Bich Street",
    "cam2": "Au Co Junction", 
    "cam3": "Tan Ky Tan Quy Street",
    "cam4": "Tan Phu Market",
    "cam5": "Dam Sen Park",
}

class SparkInsightsJob:
    def __init__(self, hdfs_namenode: str = None, hdfs_path: str = "/analytics/raw", spark_master: str = None):
        self.hdfs_namenode = hdfs_namenode or os.getenv("HDFS_NAMENODE", "hdfs-namenode:9000")
        self.hdfs_path = hdfs_path
        self.spark_master = spark_master or os.getenv("SPARK_MASTER", "local[*]")
        self.spark = None
        
        # Ensure artifacts dir exists
        os.makedirs(ARTIFACTS_DIR, exist_ok=True)

    def _init_spark(self):
        if self.spark: return self.spark
        
        logger.info(f"Initializing Spark Training Session ({self.spark_master})...")
        os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + ':/app:/app/ai_service'
        
        self.spark = SparkSession.builder \
            .appName("Violence_Camera_Credibility_Training") \
            .master(self.spark_master) \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.hadoop.fs.defaultFS", f"hdfs://{self.hdfs_namenode}") \
            .config("spark.executor.extraClassPath", "/app:/app/ai_service") \
            .config("spark.pyspark.python", "python3") \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("INFO")
        return self.spark

    def run(self) -> Dict[str, Any]:
        start_time = datetime.now()
        logger.info("STARTING CAMERA CREDIBILITY TRAINING PIPELINE")
        
        try:
            self._init_spark()
            
            # 1. Load & Preprocess Data
            df = self._load_data()
            df_processed = self._preprocess(df)
            df_processed.cache()
            count = df_processed.count()
            logger.info(f"Loaded {count} total samples")

            # Check verified data count
            verified_count = df_processed.filter(F.col("is_verified") == True).count()
            logger.info(f"Found {verified_count} verified samples")
            
            if verified_count < 100:
                logger.warning("Insufficient verified data. Need at least 100 verified samples.")
                logger.warning("Run generate_verified_scenarios.py to create synthetic training data")
                df_processed.unpersist()
                return {
                    "success": False,
                    "error": "Insufficient verified data",
                    "verified_count": verified_count,
                    "required": 100
                }
            
            # TODO: Implement Camera Credibility Training
            # Methods to implement:
            # - _compute_camera_behavior_features(df) -> DataFrame
            # - _train_kmeans_behavior_clustering(camera_features) -> List[Dict]
            # - _mine_false_alarm_patterns(df) -> List[Dict]  
            # - _train_credibility_rf(camera_features, clusters) -> Dict
            
            logger.info("Camera Credibility training not yet implemented")
            
            # Keep trends/anomalies for backward compatibility
            trends = self._compute_trends(df_processed)
            anomalies = self._detect_anomalies(df_processed)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"PIPELINE COMPLETE in {elapsed:.1f}s")
            
            df_processed.unpersist()
            
            return {
                "success": True,
                "message": "Camera Credibility training not yet implemented",
                "verified_count": verified_count,
                "training_time": elapsed,
                "trends": trends,
                "anomalies": anomalies
            }
            
        except Exception as e:
            logger.error(f"Training Pipeline Failed: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _load_data(self):
        path = f"hdfs://{self.hdfs_namenode}{self.hdfs_path}/*.csv"
        return self.spark.read.option("header", "true").option("inferSchema", "true").csv(path)

    def _preprocess(self, df: DataFrame):
        df = df.withColumn("confidence", F.col("confidence").cast("double")) \
               .withColumn("duration", F.col("duration").cast("double"))

        # 1. Convert Timestamp
        df = df.withColumn("dt", F.from_unixtime(F.col("timestamp") / 1000).cast("timestamp"))
        
        # 2. Extract Time Features
        df = df.withColumn("hour", F.hour("dt")) \
               .withColumn("day_of_week", F.dayofweek("dt") - 1) \
               .withColumn("day_name", F.date_format("dt", "EEEE"))
        
        # 3. Remove invalid rows
        df = df.filter(F.col("hour").isNotNull() & F.col("confidence").isNotNull())

        # 4. Categorize period
        df = df.withColumn("period", 
            F.when((F.col("hour") < 6) | (F.col("hour") >= 22), "Night")
            .when(F.col("hour") >= 17, "Evening")
            .when(F.col("hour") >= 12, "Afternoon")
            .otherwise("Morning")
        )
        
        # 5. Weekend boolean
        df = df.withColumn("is_weekend", F.when(F.col("day_of_week").isin([0, 6]), 1).otherwise(0))
        
        # 6. Clamp duration
        df = df.withColumn("duration", F.when(F.col("duration") > 300, 300).otherwise(F.col("duration")))
        
        logger.info(f"After preprocessing: {df.count()} valid rows")
            
        return df

    def _save_json(self, data, filename):
        path = os.path.join(ARTIFACTS_DIR, filename)
        with open(path, 'w') as f:
            json.dump(data, f, indent=2)
        logger.info(f"Saved artifact: {path}")

    # --- Legacy Helpers for Dashboard Trends ---
    def _compute_trends(self, df: DataFrame) -> Dict[str, List]:
        """Compute weekly and monthly trends using Spark SQL."""
        now = datetime.now()
        
        # Weekly trends
        current_week_start = now - timedelta(days=now.weekday())
        current_week_start = current_week_start.replace(hour=0, minute=0, second=0, microsecond=0)
        last_week_start = current_week_start - timedelta(days=7)
        
        # Monthly trends
        current_month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        last_month_start = (current_month_start - timedelta(days=1)).replace(day=1)
        
        # Convert dates to strings
        cw_str = current_week_start.strftime("%Y-%m-%d")
        lw_str = last_week_start.strftime("%Y-%m-%d")
        cm_str = current_month_start.strftime("%Y-%m-%d")
        lm_str = last_month_start.strftime("%Y-%m-%d")
        
        def get_period_counts(start_date, end_date):
            return df.filter((F.col("dt") >= start_date) & (F.col("dt") < end_date)) \
                     .groupBy("camera_id").count().collect()
        
        # 1. Weekly
        curr_counts = {r["camera_id"]: r["count"] for r in get_period_counts(cw_str, (now + timedelta(days=1)).strftime("%Y-%m-%d"))}
        prev_counts = {r["camera_id"]: r["count"] for r in get_period_counts(lw_str, cw_str)}
        
        weekly = []
        for cam_id in set(curr_counts.keys()) | set(prev_counts.keys()):
            curr = curr_counts.get(cam_id, 0)
            prev = prev_counts.get(cam_id, 0)
            change = ((curr - prev) / prev) * 100 if prev > 0 else (100.0 if curr > 0 else 0.0)
                
            weekly.append({
                "camera_id": cam_id,
                "camera_name": CAMERA_NAMES.get(cam_id, cam_id),
                "current_count": curr,
                "previous_count": prev,
                "change_pct": round(change, 1),
                "trend": "up" if change > 10 else "down" if change < -10 else "stable"
            })
        weekly.sort(key=lambda x: x["change_pct"], reverse=True)
        
        # 2. Monthly
        curr_m_counts = {r["camera_id"]: r["count"] for r in get_period_counts(cm_str, (now + timedelta(days=1)).strftime("%Y-%m-%d"))}
        prev_m_counts = {r["camera_id"]: r["count"] for r in get_period_counts(lm_str, cm_str)}
        
        monthly = []
        for cam_id in set(curr_m_counts.keys()) | set(prev_m_counts.keys()):
            curr = curr_m_counts.get(cam_id, 0)
            prev = prev_m_counts.get(cam_id, 0)
            change = ((curr - prev) / prev) * 100 if prev > 0 else (100.0 if curr > 0 else 0.0)
                
            monthly.append({
                "camera_id": cam_id,
                "camera_name": CAMERA_NAMES.get(cam_id, cam_id),
                "current_count": curr,
                "previous_count": prev,
                "change_pct": round(change, 1),
                "trend": "up" if change > 10 else "down" if change < -10 else "stable"
            })
        monthly.sort(key=lambda x: x["change_pct"], reverse=True)
        
        return {"weekly": weekly, "monthly": monthly}

    def _detect_anomalies(self, df: DataFrame) -> List[Dict]:
        """Detect anomalies using Z-score via Spark aggregation."""
        camera_counts = df.groupBy("camera_id").count()
        
        stats = camera_counts.agg(
            F.mean("count").alias("mean"),
            F.stddev("count").alias("std")
        ).collect()[0]
        
        mean_val = stats["mean"] or 0
        std_val = stats["std"] or 1
        if std_val == 0: std_val = 1
        
        results = []
        rows = camera_counts.collect()
        
        for row in rows:
            cam_id = row["camera_id"]
            count = row["count"]
            z = (count - mean_val) / std_val
            
            results.append({
                "camera_id": cam_id,
                "camera_name": CAMERA_NAMES.get(cam_id, cam_id),
                "event_count": count,
                "z_score": round(z, 2),
                "is_anomaly": z >= 1.5,
                "severity": "critical" if z >= 2.5 else "warning" if z >= 1.5 else "normal"
            })
            
        results.sort(key=lambda x: x["z_score"], reverse=True)
        return results


# Singleton Management  
_insights_job = None
_cached_results = None

def get_insights_job_instance():
    global _insights_job
    if _insights_job is None:
        _insights_job = SparkInsightsJob()
    return _insights_job

def warmup_spark():
    try:
        get_insights_job_instance()._init_spark()
    except: pass

def get_spark_insights(force_refresh: bool = False) -> Dict[str, Any]:
    """
    API Access Point: Reads pre-trained metrics from filesystem.
    Does NOT trigger training (which is heavy).
    """
    results = {
        "success": True, 
        "profiles": [], 
        "rules": [], 
        "trends": {"weekly": [], "monthly": []}, 
        "anomalies": []
    }
    
    try:
        # Load Artifacts if they exist
        # TODO: Update to load camera_credibility.json, camera_clusters.json, false_alarm_patterns.json
        return results
    except Exception as e:
        logger.error(f"Failed to load artifacts: {e}")
        return {"success": False, "error": "Insights not available. Run offline training."}

if __name__ == "__main__":
    job = SparkInsightsJob()
    res = job.run()
    print(json.dumps(res, indent=2, default=str))
