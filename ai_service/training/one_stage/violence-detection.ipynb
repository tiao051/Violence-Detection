{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1900619,"sourceType":"datasetVersion","datasetId":1132746},{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182},{"sourceId":13557741,"sourceType":"datasetVersion","datasetId":8611551}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## 1) Environment Setup\nInstalls minimal dependencies if missing. Uses AMP and saves artifacts under `/kaggle/working/violence_detection/`.\n","metadata":{}},{"cell_type":"code","source":"!pip install -qqq timm onnx onnxruntime transformers datasets huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:13:51.935432Z","iopub.execute_input":"2025-11-10T00:13:51.935617Z","iopub.status.idle":"2025-11-10T00:15:19.228725Z","shell.execute_reply.started":"2025-11-10T00:13:51.935600Z","shell.execute_reply":"2025-11-10T00:15:19.228006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime, timedelta\nimport sys, subprocess\nimport os, json, math, time, glob, hashlib\nfrom transformers import MobileViTForImageClassification\nfrom pathlib import Path\nimport math, random, gc, cv2\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom dataclasses import dataclass\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport pickle\nimport time\nimport timm\nfrom timm.data import Mixup\nfrom timm.loss import SoftTargetCrossEntropy\nfrom collections import Counter\nimport onnx, onnxruntime as ort\nBASE_DIR = '/kaggle/working/'\nBASE = Path(BASE_DIR)\nFRAMES_DIR = BASE / 'frames'\nMODELS_DIR = BASE / 'models'\nWEIGHTS_DIR = MODELS_DIR / 'weights'\nREPORTS_DIR = BASE / 'reports'\nINDEX_JSON = BASE / 'video_index.json'\nfor d in [FRAMES_DIR, WEIGHTS_DIR, REPORTS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nLABELS = {'Violence':0, 'NonViolence':1, 'PseudoViolence':2}\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:15:19.229493Z","iopub.execute_input":"2025-11-10T00:15:19.229692Z","iopub.status.idle":"2025-11-10T00:15:54.896358Z","shell.execute_reply.started":"2025-11-10T00:15:19.229671Z","shell.execute_reply":"2025-11-10T00:15:54.895734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 2) Configuration\n- Input size: **224×224**\n- Frames per second extracted: **20**\n- Epochs: **30**\n- Mixed precision: **True**\n- Temporal smoothing: **EMA** over per-frame probabilities\n- Anti-overfitting: **weight decay**, **dropout**, **label smoothing**, **strong augmentations**, **early stopping**, **gradient clipping**\n","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    # Model chọn: \"apple/mobilevit-small\" (Apple MobileViT-S) hoặc \"vit_tiny_patch16_224.augreg_in21k_ft_in1k\" (ViT-Tiny)\n    MODEL_NAME: str = \"apple/mobilevit-small\"\n    input_size: int = 256             # 256 cho MobileViT-S; đổi 224 nếu dùng ViT-Tiny\n    # Extraction\n    frames_per_second: int = 20       # GIỮ 20 FPS\n    num_classes: int = 3\n    # Train\n    epochs: int = 10\n    batch_size: int = 32\n    lr: float = 1e-4\n    weight_decay: float = 0.05\n    label_smoothing: float = 0.1\n    num_workers: int = 4\n    amp: bool = True\n    early_stopping_patience: int = 5\n    grad_clip_norm: float = 1.0\n    seed: int = 42\n\nCFG = Config()\nprint(CFG)\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:15:54.897786Z","iopub.execute_input":"2025-11-10T00:15:54.898004Z","iopub.status.idle":"2025-11-10T00:15:54.904147Z","shell.execute_reply.started":"2025-11-10T00:15:54.897987Z","shell.execute_reply":"2025-11-10T00:15:54.903246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 3) Dataset Indexing & Label Mapping\nParses all three datasets and builds an index of videos with labels and splits. AIRTLab maps **both** `violent/` and `non-violent/` to `PseudoViolence` by request.\n","metadata":{}},{"cell_type":"code","source":"random.seed(CFG.seed)\n\n# Label mapping\nLABELS = {\n    'Violence': 0,\n    'NonViolence': 1,\n    'PseudoViolence': 2,\n}\n\n# Paths (Kaggle inputs)\nPATH_HOCKEY = '/kaggle/input/hockey-fight-vidoes/data'\nPATH_RWF = '/kaggle/input/rwf2000/RWF-2000'\nPATH_AIRTLAB = '/kaggle/input/airtlab/violence-detection-dataset'\n\n# ==== TỈ LỆ CHIA ====\nTRAIN_RATIO = 0.85\nVAL_RATIO   = 0.13\nTEST_RATIO  = 0.02\n\n# ==== KHỞI TẠO INDEX ====\nindex = {\"train\": [], \"val\": [], \"test\": []}\n\n# ==== 1) RWF-2000: dùng split sẵn có (train/val) ====\nfor split in ['train', 'val']:\n    for cls in ['Fight', 'NonFight']:\n        folder = os.path.join(PATH_RWF, split, cls)\n        if not os.path.isdir(folder): \n            continue\n        for fn in sorted(os.listdir(folder)):\n            if not fn.lower().endswith('.avi'):\n                continue\n            label = 'Violence' if cls == 'Fight' else 'NonViolence'\n            item = {\n                'path': os.path.join(folder, fn),\n                'label': label,\n                'source': 'RWF-2000'\n            }\n            if split == 'train':\n                index['train'].append(item)\n            else:\n                index['val'].append(item)  # ta để nguyên vào val theo cấu trúc dataset\n\n# ==== 2) Hockey: tạo split theo 85/13/2 ====\nh_files = []\nif os.path.isdir(PATH_HOCKEY):\n    for fn in sorted(os.listdir(PATH_HOCKEY)):\n        if not fn.lower().endswith('.avi'):\n            continue\n        lbl = 'Violence' if 'fi' in fn.lower() else ('NonViolence' if 'no' in fn.lower() else None)\n        if lbl is None: \n            continue\n        h_files.append({'path': os.path.join(PATH_HOCKEY, fn), 'label': lbl, 'source': 'Hockey'})\n\nrandom.seed(CFG.seed)\nrandom.shuffle(h_files)\n\nn = len(h_files)\nn_train = int(n * TRAIN_RATIO)\nn_val   = int(n * VAL_RATIO)\nn_test  = n - n_train - n_val   # phần còn lại để đảm bảo tổng = n\n\nindex['train'] += h_files[:n_train]\nindex['val']   += h_files[n_train:n_train + n_val]\nindex['test']  += h_files[n_train + n_val:n_train + n_val + n_test]\n\nprint(f\"[Hockey] total={n} -> train={n_train}, val={n_val}, test={n_test}\")\n\n# ==== 3) AIRTLab: map violent & non-violent => PseudoViolence và chia 85/13/2 ====\na_files = []\nfor sub in ['non-violent', 'violent']:\n    subdir = os.path.join(PATH_AIRTLAB, sub)\n    if not os.path.isdir(subdir):\n        continue\n    for cam in ['cam1', 'cam2']:\n        camdir = os.path.join(subdir, cam)\n        if not os.path.isdir(camdir):\n            continue\n        for fn in sorted(os.listdir(camdir)):\n            if not (fn.lower().endswith('.mp4') or fn.lower().endswith('.avi') or fn.lower().endswith('.mov')):\n                continue\n            a_files.append({'path': os.path.join(camdir, fn), 'label': 'PseudoViolence', 'source': 'AIRTLab'})\n\nrandom.seed(42)\nrandom.shuffle(a_files)\n\nna = len(a_files)\nna_train = int(na * TRAIN_RATIO)\nna_val   = int(na * VAL_RATIO)\nna_test  = na - na_train - na_val\n\nindex['train'] += a_files[:na_train]\nindex['val']   += a_files[na_train:na_train + na_val]\nindex['test']  += a_files[na_train + na_val:na_train + na_val + na_test]\n\nprint(f\"[AIRTLab] total={na} -> train={na_train}, val={na_val}, test={na_test}\")\n\n# ==== 4) Thống kê cuối & lưu ====\nprint({\n    \"train\": len(index[\"train\"]),\n    \"val\":   len(index[\"val\"]),\n    \"test\":  len(index[\"test\"]),\n})\n\nwith open(INDEX_JSON, 'w') as f:\n    json.dump(index, f, indent=2)\n\nprint(f\"Saved index to {INDEX_JSON}\")\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:15:54.904952Z","iopub.execute_input":"2025-11-10T00:15:54.905197Z","iopub.status.idle":"2025-11-10T00:15:55.479860Z","shell.execute_reply.started":"2025-11-10T00:15:54.905176Z","shell.execute_reply":"2025-11-10T00:15:55.479254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 4) Frame Extraction (20 FPS)\nFor each video, we extract **20 frames per second**. Robust to varying FPS: selects evenly-spaced frames using timestamps.\nFrames are saved to `BASE_DIR/frames/<split>/<label>/<video_id>/frame_XXXX.jpg`.\n","metadata":{}},{"cell_type":"code","source":"def vid_hash(path): return hashlib.md5(path.encode()).hexdigest()[:12]\n\ndef letterbox_resize_bgr(frame_bgr, target_w, target_h, pad_color=(0,0,0)):\n    h, w = frame_bgr.shape[:2]\n    scale = min(target_w / w, target_h / h)\n    nw, nh = int(w * scale), int(h * scale)\n    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n    top = (target_h - nh) // 2\n    bottom = target_h - nh - top\n    left = (target_w - nw) // 2\n    right = target_w - nw - left\n    out = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=pad_color)\n    return out\n\ndef extract_resized(video_path, out_dir, target_size, jpeg_quality, overwrite, target_fps):\n    os.makedirs(out_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        tqdm.write(f\"[WARN] Cannot open: {video_path}\")\n        return 0\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0: fps = 30.0\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    duration = total_frames / fps\n\n    step_t = 1.0 / float(target_fps)\n    next_t = 0.0\n    idx = 0\n    saved = 0\n\n    # progress per video (ước lượng theo số khung cần trích)\n    expected = int(duration * target_fps)\n    while True:\n        ok, frame = cap.read()\n        if not ok: break\n        t = idx / fps\n        if t + 1e-6 >= next_t:\n            canvas = letterbox_resize_bgr(frame, target_size[0], target_size[1], pad_color=(0,0,0))\n            \n            fn_orig = out_dir / f\"frame_{saved:06d}.jpg\"\n            if overwrite or (not fn_orig.exists()):\n                cv2.imwrite(str(fn_orig), canvas, [cv2.IMWRITE_JPEG_QUALITY, int(jpeg_quality)])\n            \n            flipped = cv2.flip(canvas, 1)\n            fn_flip = out_dir / f\"frame_{saved:06d}_flip.jpg\"\n            if overwrite or (not fn_flip.exists()):\n                cv2.imwrite(str(fn_flip), flipped, [cv2.IMWRITE_JPEG_QUALITY, int(jpeg_quality)])\n\n            saved += 1\n            next_t += step_t\n        idx += 1\n        if t > duration + 1.0: break\n\n    cap.release()\n    return saved, saved\n\nwith open(INDEX_JSON, 'r') as f:\n    index = json.load(f)\n\nfor split, items in index.items():\n    print(f\"Split: {split}, videos: {len(items)}\")\n    tqdm.write(str(datetime.utcnow() + timedelta(hours=7)))\n    for it in tqdm(items, desc=f\"{split} videos\", leave=False):\n        label = it['label']\n        vid = it['path']\n        vhid = vid_hash(vid)\n        out_dir = Path(FRAMES_DIR) / split / label / vhid\n        n_orig, n_flip = extract_resized(\n            vid,\n            out_dir,\n            target_size=(CFG.input_size, CFG.input_size),\n            jpeg_quality=85,\n            overwrite=False,\n            target_fps=CFG.frames_per_second\n        )\n\nprint(\"Frame extraction done.\")\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:15:55.480584Z","iopub.execute_input":"2025-11-10T00:15:55.480846Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 5) PyTorch Dataset & Augmentations\nLoads extracted frames with strong real-time-friendly augmentations and returns tensors sized 224×224.\n","metadata":{}},{"cell_type":"code","source":"train_tfms = T.Compose([\n    T.Resize((CFG.input_size, CFG.input_size)),\n    T.RandomResizedCrop(CFG.input_size, scale=(0.7, 1.0), ratio=(0.8, 1.25)),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n    T.RandomHorizontalFlip(p=0.5),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_tfms = T.Compose([\n    T.Resize((CFG.input_size, CFG.input_size)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nclass FrameFolderDataset(Dataset):\n    def __init__(self, split, transforms=None):\n        self.items = []\n        base = os.path.join(FRAMES_DIR, split)\n        for label_name in LABELS.keys():\n            labdir = os.path.join(base, label_name)\n            if not os.path.isdir(labdir): continue\n            for vid_folder in sorted(os.listdir(labdir)):\n                full = os.path.join(labdir, vid_folder)\n                if not os.path.isdir(full): continue\n                frames = sorted(glob.glob(os.path.join(full, '*.jpg')))\n                for fr in frames:\n                    self.items.append((fr, LABELS[label_name]))\n        self.transforms = transforms\n        print(f\"Loaded {len(self.items)} frames in split={split}\")\n\n    def __len__(self): return len(self.items)\n    def __getitem__(self, idx):\n        p, y = self.items[idx]\n        img = Image.open(p).convert('RGB')\n        if self.transforms: img = self.transforms(img)\n        return img, y\n\ntrain_loader = DataLoader(FrameFolderDataset('train', transforms=train_tfms), batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\nval_loader   = DataLoader(FrameFolderDataset('val',   transforms=val_tfms),   batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\ntest_loader  = val_loader\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6) Model: ViT-Tiny (timm) with 3-way classifier\n- Lightweight ViT backbone (`vit_tiny_patch16_224`) for speed.\n- Dropout, label smoothing, cosine LR with warmup.\n- AMP training + gradient clipping.\n","metadata":{}},{"cell_type":"code","source":"def build_model(model_name, num_classes):\n    # Tạo model timm & đặt đúng số lớp\n    model = timm.create_model(\n        model_name,\n        pretrained=True,\n        num_classes=num_classes\n    )\n    return model\n\ndef build_model_hf_mobilevit(model_name, num_classes):\n    model = MobileViTForImageClassification.from_pretrained(\n        model_name,\n        num_labels=num_classes,\n        ignore_mismatched_sizes=True\n    )\n    return model\n\n# model = build_model(CFG.MODEL_NAME, CFG.num_classes)\nmodel = build_model_hf_mobilevit(CFG.MODEL_NAME, CFG.num_classes)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n\nsteps_per_epoch = max(1, len(train_loader))\ntotal_steps = steps_per_epoch * CFG.epochs\nwarmup_steps = int(0.1 * total_steps)\n\nclass CosineWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        super().__init__(optimizer, last_epoch)\n    def get_lr(self):\n        step = self.last_epoch + 1\n        lrs = []\n        for base_lr in self.base_lrs:\n            if step < self.warmup_steps:\n                lr = base_lr * step / max(1, self.warmup_steps)\n            else:\n                progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n                lr = base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n            lrs.append(lr)\n        return lrs\n\nscheduler = CosineWithWarmup(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)\nscaler = torch.cuda.amp.GradScaler(enabled=CFG.amp)\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 7) Training Loop (AMP) + Logging\nSaves:\n- `models/weights/vit_tiny_best.pth`\n- `training_history.pkl`\n- `loss.png`, `accuracy.png`\n","metadata":{}},{"cell_type":"code","source":"best_val_acc, patience, no_improve = 0.0, CFG.early_stopping_patience, 0\nhistory = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n\ndef get_logits(m, x):\n    out = m(x)\n    return out.logits if hasattr(out, \"logits\") else out  # hỗ trợ transformers & timm\ndef safe_name(name: str) -> str:\n    \"\"\"Sanitize a model name to be safe for filenames.\"\"\"\n    return (\n        name.replace('/', '__')\n            .replace('\\\\', '__')\n            .replace(':', '-')\n            .replace(' ', '_')\n    )\n\nfor epoch in tqdm(range(1, CFG.epochs+1), desc=\"Training epochs\"):\n    # ===== Train =====\n    model.train()\n    train_losses, train_preds, train_targets = [], [], []\n\n    for imgs, targets in tqdm(train_loader, desc=f\"Epoch {epoch}/{CFG.epochs} [train]\", leave=False):\n        imgs = imgs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=CFG.amp):\n            logits = get_logits(model, imgs)\n            loss = criterion(logits, targets)\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip_norm)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        train_losses.append(loss.item())\n        train_preds.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n        train_targets.extend(targets.detach().cpu().numpy().tolist())\n    tqdm.write(str(datetime.utcnow() + timedelta(hours=7)))\n    train_acc = accuracy_score(train_targets, train_preds)\n    history['train_loss'].append(float(np.mean(train_losses)))\n    history['train_acc'].append(float(train_acc))\n\n    # ===== Val =====\n    model.eval()\n    val_losses, val_preds, val_targets = [], [], []\n    with torch.no_grad():\n        for imgs, targets in tqdm(val_loader, desc=f\"Epoch {epoch} [val]\", leave=False):\n            imgs = imgs.to(device, non_blocking=True)\n            targets = targets.to(device, non_blocking=True).long()\n            with torch.cuda.amp.autocast(enabled=CFG.amp):\n                logits = get_logits(model, imgs)\n                loss = criterion(logits, targets)\n            val_losses.append(loss.item())\n            val_preds.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n            val_targets.extend(targets.detach().cpu().numpy().tolist())\n\n    val_acc = accuracy_score(val_targets, val_preds)\n    history['val_loss'].append(float(np.mean(val_losses)))\n    history['val_acc'].append(float(val_acc))\n\n    print(f\"Epoch {epoch}/{CFG.epochs} | Train Loss {history['train_loss'][-1]:.4f} Acc {train_acc:.4f} | Val Loss {history['val_loss'][-1]:.4f} Acc {val_acc:.4f}\")\n    print(datetime.utcnow() + timedelta(hours=7))\n\n    # Early stopping + save best\n    if val_acc > best_val_acc:\n        best_val_acc, no_improve = val_acc, 0        \n        best_fname = f\"{safe_name(CFG.MODEL_NAME)}_best.pth\"\n        best_path = WEIGHTS_DIR / best_fname\n        torch.save(model.state_dict(), str(best_path))\n    else:\n        no_improve += 1\n        if no_improve >= patience:\n            print('Early stopping triggered.')\n            break\n\n# Save history & plots\nwith open(os.path.join(BASE_DIR, 'training_history.pkl'), 'wb') as f:\n    pickle.dump(history, f)\n\nplt.figure(figsize=(8,4))\nplt.plot(history['train_loss'], label='train_loss')\nplt.plot(history['val_loss'], label='val_loss')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\nplt.tight_layout(); plt.savefig(os.path.join(BASE_DIR, 'loss.png')); plt.show()\n\nplt.figure(figsize=(8,4))\nplt.plot(history['train_acc'], label='train_acc')\nplt.plot(history['val_acc'], label='val_acc')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\nplt.tight_layout(); plt.savefig(os.path.join(BASE_DIR, 'accuracy.png')); plt.show()\n\nprint(\"Training finished.\")\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 8) Evaluation & Reports\nGenerates `classification_report.txt` and confusion matrix plot.\n","metadata":{}},{"cell_type":"code","source":"print(datetime.utcnow() + timedelta(hours=7))\n\nbest_w = WEIGHTS_DIR / f\"{safe_name(CFG.MODEL_NAME)}_best.pth\"\nif best_w.exists():\n    model.load_state_dict(torch.load(str(best_w), map_location=device))\nelse:\n    raise FileNotFoundError(f\"Best weights not found: {best_w}\")\nmodel.eval()\n\nall_preds, all_targets = [], []\nwith torch.no_grad():\n    for imgs, targets in tqdm(test_loader, desc=\"Evaluate [test]\"):\n        imgs = imgs.to(device)\n        with torch.cuda.amp.autocast(enabled=CFG.amp):\n            logits = model(imgs)\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        all_preds.extend(preds.tolist())\n        all_targets.extend(targets.numpy().tolist())\n\nrep = classification_report(all_targets, all_preds, target_names=list(LABELS.keys()), digits=4)\nprint(rep)\nwith open(os.path.join(BASE_DIR, 'classification_report.txt'), 'w') as f:\n    f.write(rep)\n\ncm = confusion_matrix(all_targets, all_preds)\nplt.figure(figsize=(5,4))\nplt.imshow(cm, cmap='Blues')\nplt.title('Confusion Matrix'); plt.colorbar()\nplt.xticks(range(CFG.num_classes), list(LABELS.keys()), rotation=45)\nplt.yticks(range(CFG.num_classes), list(LABELS.keys()))\nfor i in range(CFG.num_classes):\n    for j in range(CFG.num_classes):\n        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\nplt.tight_layout(); plt.savefig(os.path.join(BASE_DIR, 'confusion_matrix.png')); plt.show()\n\nprint(\"Evaluation finished.\")\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 9) Export to ONNX & Quick CPU Inference (ONNX Runtime)\nExports the trained classifier and runs a quick inference timing test on a batch.\n","metadata":{}},{"cell_type":"code","source":"print(datetime.utcnow() + timedelta(hours=7))\n\nonnx_path = WEIGHTS_DIR / f\"{safe_name(CFG.MODEL_NAME)}_best.onnx\"\ndummy = torch.randn(1, 3, CFG.input_size, CFG.input_size, device=device)\nmodel.eval()\n\ntorch.onnx.export(\n    model, dummy, str(onnx_path),\n    input_names=['input'], output_names=['logits'],\n    opset_version=17, do_constant_folding=True,\n    dynamic_axes={'input': {0: 'batch'}, 'logits': {0: 'batch'}}\n)\nprint(f\"Saved ONNX to {onnx_path}\")\n\nonnx_model = onnx.load(onnx_path)\nonnx.checker.check_model(onnx_model)\n\nsess = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\ninput_name = sess.get_inputs()[0].name\nbatch = np.random.randn(32, 3, CFG.input_size, CFG.input_size).astype(np.float32)\nstart = time.time(); out = sess.run(None, {input_name: batch})[0]\nlat = (time.time() - start) * 1000.0\nprint(f\"ONNXRuntime batch=32 latency: {lat:.2f} ms\")\n\nprint(datetime.utcnow() + timedelta(hours=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 10) Real-time Inference Pipeline (OpenCV)\nReads frames from camera/video, runs model, and applies **EMA smoothing** over per-frame probabilities to stabilize outputs.\n","metadata":{}},{"cell_type":"code","source":"infer_tfms = T.Compose([\n    T.Resize((CFG.input_size, CFG.input_size)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n@torch.no_grad()\ndef infer_frame_bgr(bgr):\n    img = Image.fromarray(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n    ten = infer_tfms(img).unsqueeze(0).to(device)\n    with torch.cuda.amp.autocast(enabled=CFG.amp):\n        logits = model(ten)\n        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n    return probs\n\n# EMA smoothing over time\nclass ProbEMA:\n    def __init__(self, alpha=CFG.ema_alpha, num_classes=CFG.num_classes):\n        self.alpha = alpha\n        self.state = np.zeros(num_classes, dtype=np.float32)\n    def update(self, probs):\n        self.state = self.alpha * probs + (1-self.alpha) * self.state\n        return self.state\n\n# Demo with a sample video (replace with 0 for webcam)\nsource = 0  # webcam; or provide a path to a video file\ncap = cv2.VideoCapture(source)\nema = ProbEMA()\n\nwhile True:\n    ok, frame = cap.read()\n    if not ok:\n        break\n    t0 = time.time()\n    probs = infer_frame_bgr(frame)\n    smoothed = ema.update(probs)\n    pred =(np.argmax(smoothed))\n    label = list(LABELS.keys())[pred]\n    latency_ms = (time.time() - t0) * 1000.0\n\n    # Overlay text\n    cv2.putText(frame, f\"Pred: {label} | latency: {latency_ms:.1f}ms\", (10, 28),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2, cv2.LINE_AA)\n    cv2.imshow('Violence Detection (ViT)', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 11) Notes on Real-time Optimizations Applied\n- **AMP (Mixed Precision)** for faster GPU throughput.\n- **ViT-Tiny** backbone for low latency.\n- **ONNX export** + **ONNX Runtime** for CPU acceleration.\n- **EMA temporal smoothing** to stabilize per-frame predictions.\n- **Strong augmentations + label smoothing + weight decay + dropout** to combat overfitting.\n- **Cosine LR with warmup**, **early stopping**, **gradient clipping** for stable training.\n- **Pinned memory & non-blocking transfers** improve input pipeline speed.\n\n> For multi-stream scalability (10–20 streams), run inference in separate threads/processes, use batched model calls, and share a single model on GPU; on CPU, prefer ONNX with `OpenVINO`/`TensorRT` if available.\n","metadata":{}}]}