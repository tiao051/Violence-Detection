name: violence-detection

services:
  # Zookeeper (for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: violence-detection-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
    ports:
      - "2181:2181"
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: violence-detection-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 40s
    networks:
      - app-network

  # Redis
  redis:
    image: redis:7-alpine
    container_name: violence-detection-redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: violence-detection-backend
    env_file:
      - ./backend/.env
    environment:
      TZ: Asia/Ho_Chi_Minh
      RTSP_BASE_URL: rtsp://rtsp-server:8554
      REDIS_URL: redis://redis:6379/0
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      MODEL_PATH: /app/ai_service/training/two-stage/checkpoints/best_model.pt
      INFERENCE_DEVICE: cpu

    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      # Persist logs on host
      - ./backend/logs:/app/logs
      - ./backend/firebase-service-account.json:/app/firebase-service-account.json
      # Mount ai_service/insights for analytics
      - ./ai_service:/app/ai_service
      - /app/__pycache__
    depends_on:
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - app-network

  # Inference Consumer (AI Model Service - SEPARATE PROCESS)
  # Runs independently to avoid GIL blocking the FastAPI backend
  # Listens to Kafka, runs model inference (distributed via Spark), publishes to Redis
  inference:
    build:
      context: ./ai_service
      dockerfile: Dockerfile
    container_name: violence-detection-inference
    env_file:
      - ./ai_service/.env
    ports:
      - "4040:4040"    # Spark Application UI (live monitoring)
    environment:
      TZ: Asia/Ho_Chi_Minh
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_FRAME_TOPIC: frames
      KAFKA_RESULT_TOPIC: inference-results
      KAFKA_CONSUMER_GROUP: inference-group
      REDIS_URL: redis://redis:6379/0
      MODEL_PATH: /app/training/two-stage/checkpoints/best_model.pt
      INFERENCE_DEVICE: cpu
      INFERENCE_BATCH_SIZE: 32
      INFERENCE_BATCH_TIMEOUT_MS: 5000
      N_SPARK_WORKERS: 4
      VIOLENCE_CONFIDENCE_THRESHOLD: 0.5
      SPARK_EVENT_LOG_DIR: /tmp/spark-events
      # Use local mode instead of cluster (avoid Python version mismatch)
      # SPARK_MASTER_URL: spark://spark-master:7077
    volumes:
      - ./ai_service:/app/ai_service
      - ./ai_service/logs:/app/ai_service/logs
      - /app/__pycache__
      - spark_logs:/tmp/spark-events
    depends_on:
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
      backend:
        condition: service_started
    # Run inference with local Spark mode
    command: >
      sh -c "
        mkdir -p /tmp/spark-events &&
        sleep 10 &&
        python -m ai_service.inference.inference_consumer_service
      "
    networks:
      - app-network
    restart: on-failure

  # RTSP Server (MediaMTX)
  rtsp-server:
    image: bluenviron/mediamtx:latest
    container_name: rtsp-server
    ports:
      - "8554:8554" # RTSP
      - "8888:8888" # HLS
      - "8889:8889" # WebRTC HTTP
      - "8189:8189/udp" # WebRTC ICE/UDP
    volumes:
      - ./mediamtx.yml:/mediamtx.yml
    environment:
      MEDIAMTX_LOGLEVEL: info
    networks:
      - app-network

  # Cameras (FFmpeg)
  camera-1:
    image: jrottenberg/ffmpeg:5.1-alpine
    container_name: camera-1
    command: >
      -re -stream_loop -1 -i /videos/violence_1.mp4
      -s 640x480 -r 15
      -c:v libx264 -preset ultrafast -tune zerolatency -bf 0 -g 30
      -an -f rtsp -rtsp_transport tcp rtsp://rtsp-server:8554/cam1
    volumes:
      - ./ai_service/utils/test_data/inputs/videos:/videos
    restart: on-failure
    networks:
      - app-network

  camera-2:
    image: jrottenberg/ffmpeg:5.1-alpine
    container_name: camera-2
    command: >
      -re -stream_loop -1 -i /videos/violence_4.mp4
      -s 640x480 -r 15
      -c:v libx264 -preset ultrafast -tune zerolatency -bf 0 -g 30
      -an -f rtsp -rtsp_transport tcp rtsp://rtsp-server:8554/cam2
    volumes:
      - ./ai_service/utils/test_data/inputs/videos:/videos
    restart: on-failure
    networks:
      - app-network

  camera-3:
    image: jrottenberg/ffmpeg:5.1-alpine
    container_name: camera-3
    command: >
      -re -stream_loop -1 -i /videos/violence_5.mp4
      -s 640x480 -r 15
      -c:v libx264 -preset ultrafast -tune zerolatency -bf 0 -g 30
      -an -f rtsp -rtsp_transport tcp rtsp://rtsp-server:8554/cam3
    volumes:
      - ./ai_service/utils/test_data/inputs/videos:/videos
    restart: on-failure
    networks:
      - app-network

  camera-4:
    image: jrottenberg/ffmpeg:5.1-alpine
    container_name: camera-4
    command: >
      -re -stream_loop -1 -i /videos/boxer.mp4
      -s 640x480 -r 15
      -c:v libx264 -preset ultrafast -tune zerolatency -bf 0 -g 30
      -an -f rtsp -rtsp_transport tcp rtsp://rtsp-server:8554/cam4
    volumes:
      - ./ai_service/utils/test_data/inputs/videos:/videos
    restart: on-failure
    networks:
      - app-network

  # Frontend (React + Vite)
  frontend:
    build:
      context: ./admin-dashboard
      dockerfile: Dockerfile
    container_name: violence-detection-frontend
    ports:
      - "5173:5173"
    volumes:
      - ./admin-dashboard:/app
      - /app/node_modules
    depends_on:
      - backend
      - rtsp-server
    networks:
      - app-network

  # Namenode - Master node that manages file system namespace
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=analytics-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
      - HDFS_CONF_dfs_replication=1
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # HDFS port
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Datanode - Worker node that stores actual data
  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:9000
      - HDFS_CONF_dfs_replication=1
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - app-network

  # ============================================================================
  # SPARK CLUSTER (Master + Workers + History Server)
  # ============================================================================
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master Port
    volumes:
      - spark_logs:/tmp/spark-events
    command: /opt/spark/sbin/start-master.sh
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: spark-worker-1
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8081"   # Spark Worker 1 Web UI
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark_logs:/tmp/spark-events
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    networks:
      - app-network

  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: spark-worker-2
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8082:8081"   # Spark Worker 2 Web UI
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - spark_logs:/tmp/spark-events
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    networks:
      - app-network

  spark-history:
    image: apache/spark:3.5.0
    container_name: spark-history-server
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/tmp/spark-events -Dspark.history.ui.port=18080
    ports:
      - "18080:18080"  # Spark History UI
    volumes:
      - spark_logs:/tmp/spark-events
    command: /opt/spark/sbin/start-history-server.sh
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - app-network

  # Analytics Consumer (HDFS Archiver)
  # Consumes inference results from Kafka and uploads to HDFS
  analytics-consumer:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: violence-detection-analytics-consumer
    env_file:
      - ./backend/.env
    environment:
      TZ: Asia/Ho_Chi_Minh
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_RESULT_TOPIC: inference-results
      HDFS_NAMENODE_URL: http://hdfs-namenode:9870
    volumes:
      - ./backend:/app
    depends_on:
      kafka:
        condition: service_healthy
      hdfs-namenode:
        condition: service_healthy
    command: python src/infrastructure/kafka/analytics_consumer.py
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  redis_data:
  hdfs_namenode:
  hdfs_datanode:
  spark_logs:
